{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using PyTorch and ODEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load China's multi-city data (S, A, I, R, D values over time)\n",
    "# Placeholder for data, replace with actual data source\n",
    "# Data format: time (t), city_name, S, A, I, R, D\n",
    "data = pd.read_csv('china_covid_data.csv')\n",
    "\n",
    "# Preprocessing data\n",
    "# Example: Extract data for a specific city\n",
    "cities = data['city_name'].unique()\n",
    "time = torch.tensor(data['time'].values, dtype=torch.float32).view(-1, 1)\n",
    "susceptible = torch.tensor(data['S'].values, dtype=torch.float32).view(-1, 1)\n",
    "asymptomatic = torch.tensor(data['A'].values, dtype=torch.float32).view(-1, 1)\n",
    "infected = torch.tensor(data['I'].values, dtype=torch.float32).view(-1, 1)\n",
    "recovered = torch.tensor(data['R'].values, dtype=torch.float32).view(-1, 1)\n",
    "deceased = torch.tensor(data['D'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the PINN model\n",
    "class SAIRD_PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAIRD_PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 5)  # Outputs: S, A, I, R, D\n",
    "        )\n",
    "        # Parameters to estimate\n",
    "        self.beta = nn.Parameter(torch.tensor(0.1))  # Initial guess for beta\n",
    "        self.gamma_A = nn.Parameter(torch.tensor(0.1))  # Initial guess for gamma_A\n",
    "        self.gamma_I = nn.Parameter(torch.tensor(0.1))  # Initial guess for gamma_I\n",
    "        self.delta_A = nn.Parameter(torch.tensor(0.01))  # Initial guess for delta_A\n",
    "        self.delta_I = nn.Parameter(torch.tensor(0.01))  # Initial guess for delta_I\n",
    "\n",
    "    def forward(self, t):\n",
    "        SAIRD = self.net(t)\n",
    "        S = SAIRD[:, 0:1]\n",
    "        A = SAIRD[:, 1:2]\n",
    "        I = SAIRD[:, 2:3]\n",
    "        R = SAIRD[:, 3:4]\n",
    "        D = SAIRD[:, 4:5]\n",
    "        return S, A, I, R, D\n",
    "\n",
    "    def compute_pde(self, t):\n",
    "        S, A, I, R, D = self.forward(t)\n",
    "        \n",
    "        # Compute derivatives using autograd\n",
    "        dS_dt = grad(S, t, torch.ones_like(S), create_graph=True)[0]\n",
    "        dA_dt = grad(A, t, torch.ones_like(A), create_graph=True)[0]\n",
    "        dI_dt = grad(I, t, torch.ones_like(I), create_graph=True)[0]\n",
    "        dR_dt = grad(R, t, torch.ones_like(R), create_graph=True)[0]\n",
    "        dD_dt = grad(D, t, torch.ones_like(D), create_graph=True)[0]\n",
    "\n",
    "        # SAIRD equations\n",
    "        pde_S = dS_dt + self.beta * S * (A + I)\n",
    "        pde_A = dA_dt - self.beta * S * (A + I) + self.gamma_A * A + self.delta_A * A\n",
    "        pde_I = dI_dt - self.gamma_A * A + self.gamma_I * I + self.delta_I * I\n",
    "        pde_R = dR_dt - self.gamma_I * I\n",
    "        pde_D = dD_dt - self.delta_A * A - self.delta_I * I\n",
    "\n",
    "        return pde_S, pde_A, pde_I, pde_R, pde_D\n",
    "\n",
    "# Instantiate the model\n",
    "model = SAIRD_PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function: Mean Squared Error\n",
    "def loss_function(t, S_data, A_data, I_data, R_data, D_data):\n",
    "    S, A, I, R, D = model(t)\n",
    "    pde_S, pde_A, pde_I, pde_R, pde_D = model.compute_pde(t)\n",
    "    \n",
    "    data_loss = (torch.mean((S - S_data) ** 2) + \n",
    "                 torch.mean((A - A_data) ** 2) + \n",
    "                 torch.mean((I - I_data) ** 2) + \n",
    "                 torch.mean((R - R_data) ** 2) + \n",
    "                 torch.mean((D - D_data) ** 2))\n",
    "    \n",
    "    pde_loss = (torch.mean(pde_S ** 2) + \n",
    "                torch.mean(pde_A ** 2) + \n",
    "                torch.mean(pde_I ** 2) + \n",
    "                torch.mean(pde_R ** 2) + \n",
    "                torch.mean(pde_D ** 2))\n",
    "    \n",
    "    return data_loss + pde_loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(time, susceptible, asymptomatic, infected, recovered, deceased)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Beta: {model.beta.item()}, '\n",
    "              f'Gamma_A: {model.gamma_A.item()}, Gamma_I: {model.gamma_I.item()}, '\n",
    "              f'Delta_A: {model.delta_A.item()}, Delta_I: {model.delta_I.item()}')\n",
    "\n",
    "# Plotting the results\n",
    "time_np = time.detach().numpy()\n",
    "S_pred, A_pred, I_pred, R_pred, D_pred = model(time)\n",
    "S_pred = S_pred.detach().numpy()\n",
    "A_pred = A_pred.detach().numpy()\n",
    "I_pred = I_pred.detach().numpy()\n",
    "R_pred = R_pred.detach().numpy()\n",
    "D_pred = D_pred.detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_np, S_pred, label='Predicted Susceptible')\n",
    "plt.plot(time_np, A_pred, label='Predicted Asymptomatic')\n",
    "plt.plot(time_np, I_pred, label='Predicted Infected')\n",
    "plt.plot(time_np, R_pred, label='Predicted Recovered')\n",
    "plt.plot(time_np, D_pred, label='Predicted Deceased')\n",
    "plt.scatter(time_np, susceptible, label='Actual Susceptible', s=10)\n",
    "plt.scatter(time_np, asymptomatic, label='Actual Asymptomatic', s=10)\n",
    "plt.scatter(time_np, infected, label='Actual Infected', s=10)\n",
    "plt.scatter(time_np, recovered, label='Actual Recovered', s=10)\n",
    "plt.scatter(time_np, deceased, label='Actual Deceased', s=10)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated Beta: {model.beta.item()}\")\n",
    "print(f\"Estimated Gamma_A: {model.gamma_A.item()}\")\n",
    "print(f\"Estimated Gamma_I: {model.gamma_I.item()}\")\n",
    "print(f\"Estimated Delta_A: {model.delta_A.item()}\")\n",
    "print(f\"Estimated Delta_I: {model.delta_I.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,\n",
       "       nan, nan])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('China_deaths.csv')\n",
    "df = df.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "\n",
    "country_infections = df.iloc[0, 1:].values.astype(float)\n",
    "country_infections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.214551623911342e+17, Beta: 0.09999928623437881, Gamma_A: 0.10100000351667404, Gamma_I: 0.0989999994635582, Delta_A: 0.009000000543892384, Delta_I: 0.009000000543892384\n",
      "Epoch 500, Loss: 2.2145512803139584e+17, Beta: -0.390389084815979, Gamma_A: 0.04876697435975075, Gamma_I: 5.9420635807327926e-05, Delta_A: -0.048775993287563324, Delta_I: 0.050099220126867294\n",
      "Epoch 1000, Loss: 2.2145511085152666e+17, Beta: -0.8205052614212036, Gamma_A: 0.04929366335272789, Gamma_I: 1.8500370060792193e-05, Delta_A: -0.04937499761581421, Delta_I: 0.04914756491780281\n",
      "Epoch 1500, Loss: 2.214550593119191e+17, Beta: -1.0674132108688354, Gamma_A: 0.04967473819851875, Gamma_I: -7.412245395244099e-06, Delta_A: -0.04959115386009216, Delta_I: 0.04864512011408806\n",
      "Epoch 2000, Loss: 2.2145504213204992e+17, Beta: -1.182985782623291, Gamma_A: 0.04990466684103012, Gamma_I: -1.808617525966838e-05, Delta_A: -0.049926161766052246, Delta_I: 0.048195842653512955\n",
      "Epoch 2500, Loss: 2.2145500777231155e+17, Beta: -1.2393261194229126, Gamma_A: 0.05012470856308937, Gamma_I: -2.7932069315284025e-06, Delta_A: -0.05012135207653046, Delta_I: 0.047860026359558105\n",
      "Epoch 3000, Loss: 2.2145499059244237e+17, Beta: -1.2646230459213257, Gamma_A: 0.0503622367978096, Gamma_I: -3.8941499951761216e-05, Delta_A: -0.0503455214202404, Delta_I: 0.047490011900663376\n",
      "Epoch 3500, Loss: 2.214549734125732e+17, Beta: -1.2711073160171509, Gamma_A: 0.050545912235975266, Gamma_I: -1.3016879165661521e-05, Delta_A: -0.050519175827503204, Delta_I: 0.047208309173583984\n",
      "Epoch 4000, Loss: 2.2145492187296563e+17, Beta: -1.2650320529937744, Gamma_A: 0.050779905170202255, Gamma_I: -6.627234688494354e-05, Delta_A: -0.05076222121715546, Delta_I: 0.046838946640491486\n",
      "Epoch 4500, Loss: 2.2145488751322726e+17, Beta: -1.2498679161071777, Gamma_A: 0.05088837444782257, Gamma_I: 3.3994489058386534e-05, Delta_A: -0.05094395950436592, Delta_I: 0.046633489429950714\n",
      "Epoch 5000, Loss: 2.214548531534889e+17, Beta: -1.2289828062057495, Gamma_A: 0.05114055052399635, Gamma_I: -8.800378964224365e-06, Delta_A: -0.05113941431045532, Delta_I: 0.046288006007671356\n",
      "Epoch 5500, Loss: 2.214548359736197e+17, Beta: -1.2026652097702026, Gamma_A: 0.05144783481955528, Gamma_I: -0.00012491770030464977, Delta_A: -0.05143067240715027, Delta_I: 0.045839227735996246\n",
      "Epoch 6000, Loss: 2.2145480161388134e+17, Beta: -1.1708457469940186, Gamma_A: 0.05155457928776741, Gamma_I: -4.799449015990831e-06, Delta_A: -0.05167124420404434, Delta_I: 0.04560380056500435\n",
      "Epoch 6500, Loss: 2.2145476725414298e+17, Beta: -1.1379212141036987, Gamma_A: 0.05179129168391228, Gamma_I: 4.5181994209997356e-05, Delta_A: -0.051811583340168, Delta_I: 0.04531126469373703\n",
      "Epoch 7000, Loss: 2.214547500742738e+17, Beta: -1.0998222827911377, Gamma_A: 0.052118975669145584, Gamma_I: -2.1590940377791412e-06, Delta_A: -0.052089329808950424, Delta_I: 0.04486602544784546\n",
      "Epoch 7500, Loss: 2.2145469853466624e+17, Beta: -1.0608913898468018, Gamma_A: 0.05242103338241577, Gamma_I: 6.240979928406887e-07, Delta_A: -0.052427444607019424, Delta_I: 0.04441004619002342\n",
      "Epoch 8000, Loss: 2.2145468135479706e+17, Beta: -1.0196603536605835, Gamma_A: 0.05283414572477341, Gamma_I: -5.261499609332532e-05, Delta_A: -0.052785370498895645, Delta_I: 0.04385232925415039\n",
      "Epoch 8500, Loss: 2.214546298151895e+17, Beta: -0.973145604133606, Gamma_A: 0.05320364981889725, Gamma_I: 1.0575856776995352e-06, Delta_A: -0.053206343203783035, Delta_I: 0.04332388937473297\n",
      "Epoch 9000, Loss: 2.214546298151895e+17, Beta: -0.9246283769607544, Gamma_A: 0.05370787903666496, Gamma_I: -9.952411346603185e-06, Delta_A: -0.05370619520545006, Delta_I: 0.042649272829294205\n",
      "Epoch 9500, Loss: 2.2145459545545114e+17, Beta: -0.8716694712638855, Gamma_A: 0.05430750921368599, Gamma_I: -1.938863169925753e-05, Delta_A: -0.054258882999420166, Delta_I: 0.04188917949795723\n",
      "Epoch 10000, Loss: 2.214545095561052e+17, Beta: -0.8148424029350281, Gamma_A: 0.054954420775175095, Gamma_I: 1.051035724231042e-06, Delta_A: -0.054958343505859375, Delta_I: 0.041043296456336975\n",
      "Epoch 10500, Loss: 2.2145449237623603e+17, Beta: -0.7554041743278503, Gamma_A: 0.05575644597411156, Gamma_I: -5.4763833759352565e-05, Delta_A: -0.05578815937042236, Delta_I: 0.04003489762544632\n",
      "Epoch 11000, Loss: 2.2145445801649766e+17, Beta: -0.6829448342323303, Gamma_A: 0.05655776336789131, Gamma_I: -4.4946639832232904e-07, Delta_A: -0.05656035616993904, Delta_I: 0.03910510241985321\n",
      "Epoch 11500, Loss: 2.2145444083662848e+17, Beta: -0.5974833965301514, Gamma_A: 0.05743645131587982, Gamma_I: -5.487736416398548e-07, Delta_A: -0.05743822827935219, Delta_I: 0.03810464218258858\n",
      "Epoch 12000, Loss: 2.214544236567593e+17, Beta: -0.5037965178489685, Gamma_A: 0.05832439661026001, Gamma_I: -2.2891683329362422e-05, Delta_A: -0.058350756764411926, Delta_I: 0.03709941729903221\n",
      "Epoch 12500, Loss: 2.2145437211715174e+17, Beta: -0.399386465549469, Gamma_A: 0.059160321950912476, Gamma_I: -9.799159670365043e-07, Delta_A: -0.05916205793619156, Delta_I: 0.0361911840736866\n",
      "Epoch 13000, Loss: 2.2145433775741338e+17, Beta: -0.28783535957336426, Gamma_A: 0.05997570976614952, Gamma_I: -2.4224524963756267e-07, Delta_A: -0.059976886957883835, Delta_I: 0.03530574589967728\n",
      "Epoch 13500, Loss: 2.214543205775442e+17, Beta: -0.18157073855400085, Gamma_A: 0.060763806104660034, Gamma_I: -8.842755960358772e-06, Delta_A: -0.060764241963624954, Delta_I: 0.034456539899110794\n",
      "Epoch 14000, Loss: 2.2145426903793664e+17, Beta: -0.09609539806842804, Gamma_A: 0.06151155009865761, Gamma_I: -1.0549814760452136e-05, Delta_A: -0.061511557549238205, Delta_I: 0.0336562879383564\n",
      "Epoch 14500, Loss: 2.2145425185806746e+17, Beta: -0.04179450124502182, Gamma_A: 0.06221127137541771, Gamma_I: 3.5258783555036644e-06, Delta_A: -0.06221156567335129, Delta_I: 0.0329100638628006\n",
      "Epoch 15000, Loss: 2.2145423467819827e+17, Beta: -0.017016248777508736, Gamma_A: 0.06289342790842056, Gamma_I: 3.0951400731282774e-06, Delta_A: -0.06289352476596832, Delta_I: 0.03218511864542961\n",
      "Epoch 15500, Loss: 2.2145418313859072e+17, Beta: -0.009439277462661266, Gamma_A: 0.06354909390211105, Gamma_I: -7.42677116249979e-07, Delta_A: -0.0635492131114006, Delta_I: 0.031489595770835876\n",
      "Epoch 16000, Loss: 2.2145416595872154e+17, Beta: -0.007675064727663994, Gamma_A: 0.06417262554168701, Gamma_I: -2.1233203995052463e-07, Delta_A: -0.06417274475097656, Delta_I: 0.03082907199859619\n",
      "Epoch 16500, Loss: 2.2145413159898317e+17, Beta: -0.006936181336641312, Gamma_A: 0.06479842960834503, Gamma_I: -2.947459142887965e-05, Delta_A: -0.0647987574338913, Delta_I: 0.030167676508426666\n",
      "Epoch 17000, Loss: 2.214540972392448e+17, Beta: -0.006594324484467506, Gamma_A: 0.06535057723522186, Gamma_I: -6.650057002843823e-06, Delta_A: -0.06535074859857559, Delta_I: 0.029585497453808784\n",
      "Epoch 17500, Loss: 2.214540800593756e+17, Beta: -0.0061551895923912525, Gamma_A: 0.06589522212743759, Gamma_I: -4.540995064417075e-07, Delta_A: -0.06589532643556595, Delta_I: 0.02901189774274826\n",
      "Epoch 18000, Loss: 2.2145404569963725e+17, Beta: -0.0055094617418944836, Gamma_A: 0.06657609343528748, Gamma_I: -0.00016104907263070345, Delta_A: -0.06657543778419495, Delta_I: 0.02829587832093239\n",
      "Epoch 18500, Loss: 2.2145401133989888e+17, Beta: -0.005296827759593725, Gamma_A: 0.06697254627943039, Gamma_I: -4.323801476857625e-05, Delta_A: -0.06697246432304382, Delta_I: 0.02787996269762516\n",
      "Epoch 19000, Loss: 2.2145401133989888e+17, Beta: -0.00503940973430872, Gamma_A: 0.06742148846387863, Gamma_I: -1.981395143957343e-06, Delta_A: -0.06742154806852341, Delta_I: 0.027408933266997337\n",
      "Epoch 19500, Loss: 2.2145394262042214e+17, Beta: -0.004578608553856611, Gamma_A: 0.06788944453001022, Gamma_I: -7.684064939894597e-08, Delta_A: -0.06788952648639679, Delta_I: 0.026918767020106316\n",
      "Epoch 20000, Loss: 2.2145392544055296e+17, Beta: -0.00431388895958662, Gamma_A: 0.06834530085325241, Gamma_I: -4.051963514939416e-06, Delta_A: -0.06834535300731659, Delta_I: 0.02644169330596924\n",
      "Epoch 20500, Loss: 2.214538910808146e+17, Beta: -0.004325253423303366, Gamma_A: 0.06876545399427414, Gamma_I: 1.1799545973190106e-05, Delta_A: -0.0687655508518219, Delta_I: 0.0260025542229414\n",
      "Epoch 21000, Loss: 2.214538739009454e+17, Beta: -0.004021554719656706, Gamma_A: 0.06919298321008682, Gamma_I: 3.374143943801755e-06, Delta_A: -0.06919310241937637, Delta_I: 0.025556355714797974\n",
      "Epoch 21500, Loss: 2.2145383954120704e+17, Beta: -0.00387544184923172, Gamma_A: 0.06959991902112961, Gamma_I: 5.333237140803249e-07, Delta_A: -0.06959997862577438, Delta_I: 0.02513204514980316\n",
      "Epoch 22000, Loss: 2.2145382236133786e+17, Beta: -0.0035349801182746887, Gamma_A: 0.06998997926712036, Gamma_I: -1.6904991184674145e-08, Delta_A: -0.06999005377292633, Delta_I: 0.02472553960978985\n",
      "Epoch 22500, Loss: 2.2145375364186112e+17, Beta: -0.003171342657878995, Gamma_A: 0.07040462642908096, Gamma_I: -4.110171357751824e-05, Delta_A: -0.07040438801050186, Delta_I: 0.024293551221489906\n",
      "Epoch 23000, Loss: 2.2145373646199194e+17, Beta: -0.0033464357256889343, Gamma_A: 0.07071024179458618, Gamma_I: 1.796421747712884e-05, Delta_A: -0.07071039080619812, Delta_I: 0.023975621908903122\n",
      "Epoch 23500, Loss: 2.2145370210225357e+17, Beta: -0.003064384451135993, Gamma_A: 0.07107766717672348, Gamma_I: -8.56305973684357e-07, Delta_A: -0.07107774913311005, Delta_I: 0.023593803867697716\n",
      "Epoch 24000, Loss: 2.214536677425152e+17, Beta: -0.002775719156488776, Gamma_A: 0.07141311466693878, Gamma_I: 5.829389806422114e-07, Delta_A: -0.07141321897506714, Delta_I: 0.023245345801115036\n",
      "Epoch 24500, Loss: 2.21453650562646e+17, Beta: -0.002632433082908392, Gamma_A: 0.07173778861761093, Gamma_I: 5.845566306561523e-07, Delta_A: -0.0717378482222557, Delta_I: 0.022908175364136696\n",
      "Epoch 25000, Loss: 2.2145363338277683e+17, Beta: -0.00247076409868896, Gamma_A: 0.07205144315958023, Gamma_I: -1.4174233342600928e-07, Delta_A: -0.072051502764225, Delta_I: 0.022582542151212692\n",
      "Epoch 25500, Loss: 2.2145359902303846e+17, Beta: -0.00233347131870687, Gamma_A: 0.07235272973775864, Gamma_I: 3.996646569248696e-08, Delta_A: -0.0723528042435646, Delta_I: 0.02226976491510868\n",
      "Epoch 26000, Loss: 2.214535646633001e+17, Beta: -0.0022093968000262976, Gamma_A: 0.07264308631420135, Gamma_I: -3.3795956255744386e-08, Delta_A: -0.07264317572116852, Delta_I: 0.021968422457575798\n",
      "Epoch 26500, Loss: 2.2145353030356173e+17, Beta: -0.0020933107007294893, Gamma_A: 0.07292231172323227, Gamma_I: -3.523368619084977e-08, Delta_A: -0.07292238622903824, Delta_I: 0.021678652614355087\n",
      "Epoch 27000, Loss: 2.2145349594382336e+17, Beta: -0.0019288385519757867, Gamma_A: 0.07319051772356033, Gamma_I: 1.0940961203687039e-07, Delta_A: -0.07319056987762451, Delta_I: 0.021400297060608864\n",
      "Epoch 27500, Loss: 2.2145347876395418e+17, Beta: -0.0017628862988203764, Gamma_A: 0.07343902438879013, Gamma_I: 9.388108082930557e-06, Delta_A: -0.0734391063451767, Delta_I: 0.021142324432730675\n",
      "Epoch 28000, Loss: 2.21453461584085e+17, Beta: -0.0019468455575406551, Gamma_A: 0.07373107969760895, Gamma_I: -3.791147901210934e-05, Delta_A: -0.07373113185167313, Delta_I: 0.020839011296629906\n",
      "Epoch 28500, Loss: 2.2145341004447744e+17, Beta: -0.0017522842390462756, Gamma_A: 0.07393745332956314, Gamma_I: -7.900014679762535e-06, Delta_A: -0.07393750548362732, Delta_I: 0.020624499768018723\n",
      "Epoch 29000, Loss: 2.2145339286460826e+17, Beta: -0.001574325026012957, Gamma_A: 0.07415533065795898, Gamma_I: -1.0566316177573754e-06, Delta_A: -0.07415539771318436, Delta_I: 0.020398609340190887\n",
      "Epoch 29500, Loss: 2.214533585048699e+17, Beta: -0.0014950759941712022, Gamma_A: 0.07436808943748474, Gamma_I: -1.039190237861476e-06, Delta_A: -0.07436814904212952, Delta_I: 0.020178068429231644\n",
      "Epoch 30000, Loss: 2.2145332414513152e+17, Beta: -0.0014078621752560139, Gamma_A: 0.07457447797060013, Gamma_I: -7.157549589464907e-06, Delta_A: -0.07457451522350311, Delta_I: 0.019964225590229034\n",
      "Epoch 30500, Loss: 2.2145328978539315e+17, Beta: -0.001333614345639944, Gamma_A: 0.07475369423627853, Gamma_I: 9.168361430056393e-07, Delta_A: -0.07475374639034271, Delta_I: 0.019778309389948845\n",
      "Epoch 31000, Loss: 2.2145327260552397e+17, Beta: -0.0011911068577319384, Gamma_A: 0.07487855851650238, Gamma_I: 5.011706525692716e-05, Delta_A: -0.07487867027521133, Delta_I: 0.019648555666208267\n",
      "Epoch 31500, Loss: 2.214532382457856e+17, Beta: -0.0009851594222709537, Gamma_A: 0.07507123053073883, Gamma_I: 1.3328488421393558e-05, Delta_A: -0.0750712901353836, Delta_I: 0.01944945938885212\n",
      "Epoch 32000, Loss: 2.2145320388604723e+17, Beta: -0.0008698745514266193, Gamma_A: 0.07523678243160248, Gamma_I: -1.3924523955211043e-05, Delta_A: -0.07523684948682785, Delta_I: 0.01927858591079712\n",
      "Epoch 32500, Loss: 2.2145316952630886e+17, Beta: -0.0007120576919987798, Gamma_A: 0.07534585893154144, Gamma_I: -4.25464259024011e-06, Delta_A: -0.07534588873386383, Delta_I: 0.019165625795722008\n",
      "Epoch 33000, Loss: 2.2145315234643968e+17, Beta: -0.000572718505281955, Gamma_A: 0.07543172687292099, Gamma_I: 5.844124643772375e-06, Delta_A: -0.07543174922466278, Delta_I: 0.019076433032751083\n",
      "Epoch 33500, Loss: 2.214531179867013e+17, Beta: -0.000318447157042101, Gamma_A: 0.07550942152738571, Gamma_I: -5.77792718559067e-07, Delta_A: -0.0755094587802887, Delta_I: 0.01899571716785431\n",
      "Epoch 34000, Loss: 2.2145308362696294e+17, Beta: -0.00021661771461367607, Gamma_A: 0.07555420696735382, Gamma_I: 1.895121272355027e-06, Delta_A: -0.07555422931909561, Delta_I: 0.018948538228869438\n",
      "Epoch 34500, Loss: 2.2145304926722458e+17, Beta: -6.234473403310403e-05, Gamma_A: 0.07556849718093872, Gamma_I: 1.391747900925111e-05, Delta_A: -0.07556851953268051, Delta_I: 0.018932264298200607\n",
      "Epoch 35000, Loss: 2.214530320873554e+17, Beta: -0.00012689105642493814, Gamma_A: 0.07559358328580856, Gamma_I: 1.4507729417800874e-07, Delta_A: -0.07559357583522797, Delta_I: 0.018905455246567726\n",
      "Epoch 35500, Loss: 2.2145299772761702e+17, Beta: -1.1703930795192719e-05, Gamma_A: 0.07559667527675629, Gamma_I: -2.176888926896936e-07, Delta_A: -0.0755966529250145, Delta_I: 0.018900396302342415\n",
      "Epoch 36000, Loss: 2.2145296336787866e+17, Beta: 2.927697278209962e-05, Gamma_A: 0.07559593766927719, Gamma_I: 2.046204485850467e-07, Delta_A: -0.07559596002101898, Delta_I: 0.018899444490671158\n",
      "Epoch 36500, Loss: 2.214529290081403e+17, Beta: -6.393853982444853e-05, Gamma_A: 0.07560517638921738, Gamma_I: -1.065128617483424e-05, Delta_A: -0.07560516893863678, Delta_I: 0.01888807862997055\n",
      "Epoch 37000, Loss: 2.214529118282711e+17, Beta: -0.0001278537092730403, Gamma_A: 0.07559289038181305, Gamma_I: 8.63796913108672e-07, Delta_A: -0.07559287548065186, Delta_I: 0.018899383023381233\n",
      "Epoch 37500, Loss: 2.2145286028866355e+17, Beta: -3.611613647080958e-05, Gamma_A: 0.0755142793059349, Gamma_I: 7.813050615368411e-05, Delta_A: -0.07551427185535431, Delta_I: 0.01897629350423813\n",
      "Epoch 38000, Loss: 2.2145284310879437e+17, Beta: -4.6111126721370965e-05, Gamma_A: 0.07559074461460114, Gamma_I: 7.358671894053259e-08, Delta_A: -0.07559072971343994, Delta_I: 0.018897831439971924\n",
      "Epoch 38500, Loss: 2.21452808749056e+17, Beta: 9.240108192898333e-05, Gamma_A: 0.07558920979499817, Gamma_I: -1.5702276812135096e-08, Delta_A: -0.07558923959732056, Delta_I: 0.018897324800491333\n",
      "Epoch 39000, Loss: 2.214527915691868e+17, Beta: -4.5532684453064576e-05, Gamma_A: 0.07558786869049072, Gamma_I: -1.9066188627903102e-08, Delta_A: -0.07558785378932953, Delta_I: 0.018896974623203278\n",
      "Epoch 39500, Loss: 2.2145277438931763e+17, Beta: 3.494305565254763e-05, Gamma_A: 0.07558619230985641, Gamma_I: 7.519713562942343e-08, Delta_A: -0.07558620721101761, Delta_I: 0.018896648660302162\n",
      "Epoch 40000, Loss: 2.2145272284971008e+17, Beta: -0.00012634709128178656, Gamma_A: 0.07558488845825195, Gamma_I: 6.069738134328873e-08, Delta_A: -0.07558486610651016, Delta_I: 0.018896309658885002\n",
      "Epoch 40500, Loss: 2.214526884899717e+17, Beta: 0.00011036658543162048, Gamma_A: 0.07558353990316391, Gamma_I: -8.279220864437775e-09, Delta_A: -0.0755835697054863, Delta_I: 0.018895884975790977\n",
      "Epoch 41000, Loss: 2.2145265413023334e+17, Beta: 3.222589293727651e-05, Gamma_A: 0.07558180391788483, Gamma_I: 2.0409160583767516e-07, Delta_A: -0.07558181136846542, Delta_I: 0.01889570616185665\n",
      "Epoch 41500, Loss: 2.2145263695036416e+17, Beta: 9.455615509068593e-05, Gamma_A: 0.075580894947052, Gamma_I: -2.73552245744213e-07, Delta_A: -0.075580894947052, Delta_I: 0.01889488659799099\n",
      "Epoch 42000, Loss: 2.214526025906258e+17, Beta: 5.8277124480810016e-05, Gamma_A: 0.0755273774266243, Gamma_I: 5.2181425417074934e-05, Delta_A: -0.0755273774266243, Delta_I: 0.018947072327136993\n",
      "Epoch 42500, Loss: 2.2145256823088742e+17, Beta: 1.3826691429130733e-05, Gamma_A: 0.07557784020900726, Gamma_I: 9.907749642934505e-08, Delta_A: -0.07557783275842667, Delta_I: 0.01889459602534771\n",
      "Epoch 43000, Loss: 2.214524995114107e+17, Beta: -8.011836325749755e-05, Gamma_A: 0.07557657361030579, Gamma_I: -1.244160046098841e-07, Delta_A: -0.07557656615972519, Delta_I: 0.018893981352448463\n",
      "Epoch 43500, Loss: 2.214524823315415e+17, Beta: -6.823868898209184e-05, Gamma_A: 0.07557468116283417, Gamma_I: -6.7442069706658e-09, Delta_A: -0.07557468116283417, Delta_I: 0.018893657252192497\n",
      "Epoch 44000, Loss: 2.2145243079193395e+17, Beta: -7.013105641817674e-05, Gamma_A: 0.07557522505521774, Gamma_I: -2.0905167730234098e-06, Delta_A: -0.07557521760463715, Delta_I: 0.01889118365943432\n",
      "Epoch 44500, Loss: 2.2145243079193395e+17, Beta: -4.078887286596e-05, Gamma_A: 0.0755767822265625, Gamma_I: -5.1469442041707225e-06, Delta_A: -0.0755767896771431, Delta_I: 0.018887754529714584\n",
      "Epoch 45000, Loss: 2.214523964321956e+17, Beta: 5.805015825899318e-05, Gamma_A: 0.07556617259979248, Gamma_I: 4.057707883475814e-06, Delta_A: -0.07556618750095367, Delta_I: 0.018896616995334625\n",
      "Epoch 45500, Loss: 2.214523620724572e+17, Beta: -1.8297992937732488e-05, Gamma_A: 0.07561849057674408, Gamma_I: -4.9485795898362994e-05, Delta_A: -0.07561847567558289, Delta_I: 0.01884276419878006\n",
      "Epoch 46000, Loss: 2.2145231053284966e+17, Beta: -6.301292887656018e-05, Gamma_A: 0.07556881010532379, Gamma_I: -9.402764931110141e-07, Delta_A: -0.0755687803030014, Delta_I: 0.018891027197241783\n",
      "Epoch 46500, Loss: 2.2145229335298048e+17, Beta: 1.0485419807082508e-05, Gamma_A: 0.07556629925966263, Gamma_I: -6.4182520409517e-11, Delta_A: -0.07556629925966263, Delta_I: 0.018891574814915657\n",
      "Epoch 47000, Loss: 2.214522589932421e+17, Beta: -2.2480169718619436e-05, Gamma_A: 0.07556464523077011, Gamma_I: 6.96308610770302e-09, Delta_A: -0.07556463032960892, Delta_I: 0.01889117620885372\n",
      "Epoch 47500, Loss: 2.2145224181337293e+17, Beta: -0.0001526680716779083, Gamma_A: 0.07556169480085373, Gamma_I: 1.2240827800269471e-06, Delta_A: -0.07556167244911194, Delta_I: 0.01889195293188095\n",
      "Epoch 48000, Loss: 2.2145222463350374e+17, Beta: 8.424938641837798e-06, Gamma_A: 0.07554584741592407, Gamma_I: 1.5674242604291067e-05, Delta_A: -0.07554585486650467, Delta_I: 0.01890605315566063\n",
      "Epoch 48500, Loss: 2.214521730938962e+17, Beta: 8.96483106771484e-05, Gamma_A: 0.07556048780679703, Gamma_I: -1.9378246562951063e-08, Delta_A: -0.07556048780679703, Delta_I: 0.01889011077582836\n",
      "Epoch 49000, Loss: 2.2145213873415782e+17, Beta: -4.5835702621843666e-05, Gamma_A: 0.07555876672267914, Gamma_I: 1.2286243133985408e-08, Delta_A: -0.07555875927209854, Delta_I: 0.018889708444476128\n",
      "Epoch 49500, Loss: 2.2145212155428864e+17, Beta: -4.0117687603924423e-05, Gamma_A: 0.07555467635393143, Gamma_I: 2.3900788619357627e-06, Delta_A: -0.07555466890335083, Delta_I: 0.018891656771302223\n",
      "\n",
      "Final Estimated Parameters:\n",
      "Estimated Beta: 4.39148279838264e-05\n",
      "Estimated Gamma_A: 0.07555672526359558\n",
      "Estimated Gamma_I: -1.236291836903547e-06\n",
      "Estimated Delta_A: -0.07555675506591797\n",
      "Estimated Delta_I: 0.018887627869844437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset for I, R, and D\n",
    "infected_data = pd.read_csv('China_infections.csv')  # Replace with your file path\n",
    "infected_data = infected_data.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "recovered_data = pd.read_csv('China_recoveries.csv')  # Replace with your file path\n",
    "recovered_data = recovered_data.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "deaths_data = pd.read_csv('China_deaths.csv')  # Replace with your file path\n",
    "deaths_data = deaths_data.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "\n",
    "# Define the total population for all cities combined\n",
    "total_population = 1411778724/3  # Approximate population of China\n",
    "\n",
    "# Sum up the infected, recovered, and deceased cases across all cities for each day\n",
    "# I_data = infected_data.iloc[:, 1:].sum(axis=0).values\n",
    "# R_data = recovered_data.iloc[:, 1:].sum(axis=0).values\n",
    "# D_data = deaths_data.iloc[:, 1:].sum(axis=0).values\n",
    "# time = np.arange(len(I_data))\n",
    "\n",
    "I_data = pd.to_numeric(infected_data.iloc[:, 1:].sum(axis=0), errors='coerce').fillna(0).values\n",
    "R_data = pd.to_numeric(recovered_data.iloc[:, 1:].sum(axis=0), errors='coerce').fillna(0).values\n",
    "D_data = pd.to_numeric(deaths_data.iloc[:, 1:].sum(axis=0), errors='coerce').fillna(0).values\n",
    "time = np.arange(len(I_data))\n",
    "\n",
    "# Convert data to torch tensors\n",
    "time = torch.tensor(time, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
    "I_data = torch.tensor(I_data, dtype=torch.float32).view(-1, 1)\n",
    "R_data = torch.tensor(R_data, dtype=torch.float32).view(-1, 1)\n",
    "D_data = torch.tensor(D_data, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Assume alpha for A estimation (e.g., 0.5)\n",
    "alpha = 0.25\n",
    "A_data = alpha * I_data\n",
    "\n",
    "# Estimate initial S based on total population N\n",
    "N = total_population\n",
    "S_data = N - I_data - R_data - D_data\n",
    "\n",
    "# Define the PINN model for SAIRD\n",
    "class SAIRD_PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAIRD_PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 5)  # Outputs: S, A, I, R, D\n",
    "        )\n",
    "        # Parameters to estimate\n",
    "        self.beta = nn.Parameter(torch.tensor(0.1))  # Initial guess for beta\n",
    "        self.gamma_A = nn.Parameter(torch.tensor(0.1))  # Initial guess for gamma_A\n",
    "        self.gamma_I = nn.Parameter(torch.tensor(0.1))  # Initial guess for gamma_I\n",
    "        self.delta_A = nn.Parameter(torch.tensor(0.01))  # Initial guess for delta_A\n",
    "        self.delta_I = nn.Parameter(torch.tensor(0.01))  # Initial guess for delta_I\n",
    "\n",
    "    def forward(self, t):\n",
    "        SAIRD = self.net(t)\n",
    "        S = SAIRD[:, 0:1]\n",
    "        A = SAIRD[:, 1:2]\n",
    "        I = SAIRD[:, 2:3]\n",
    "        R = SAIRD[:, 3:4]\n",
    "        D = SAIRD[:, 4:5]\n",
    "        return S, A, I, R, D\n",
    "\n",
    "    def compute_pde(self, t):\n",
    "        S, A, I, R, D = self.forward(t)\n",
    "        \n",
    "        # Compute derivatives using autograd\n",
    "        dS_dt = grad(S, t, torch.ones_like(S), create_graph=True)[0]\n",
    "        dA_dt = grad(A, t, torch.ones_like(A), create_graph=True)[0]\n",
    "        dI_dt = grad(I, t, torch.ones_like(I), create_graph=True)[0]\n",
    "        dR_dt = grad(R, t, torch.ones_like(R), create_graph=True)[0]\n",
    "        dD_dt = grad(D, t, torch.ones_like(D), create_graph=True)[0]\n",
    "\n",
    "        # SAIRD equations\n",
    "        pde_S = dS_dt + self.beta * S * (A + I) / N\n",
    "        pde_A = dA_dt - self.beta * S * (A + I) / N + self.gamma_A * A + self.delta_A * A\n",
    "        pde_I = dI_dt - self.gamma_A * A + self.gamma_I * I + self.delta_I * I\n",
    "        pde_R = dR_dt - self.gamma_I * I\n",
    "        pde_D = dD_dt - self.delta_A * A - self.delta_I * I\n",
    "\n",
    "        return pde_S, pde_A, pde_I, pde_R, pde_D\n",
    "\n",
    "# Instantiate the model\n",
    "model = SAIRD_PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(t, S_data, A_data, I_data, R_data, D_data):\n",
    "    S, A, I, R, D = model(t)\n",
    "    pde_S, pde_A, pde_I, pde_R, pde_D = model.compute_pde(t)\n",
    "    \n",
    "    data_loss = (torch.mean((S - S_data) ** 2) + \n",
    "                 torch.mean((A - A_data) ** 2) + \n",
    "                 torch.mean((I - I_data) ** 2) + \n",
    "                 torch.mean((R - R_data) ** 2) + \n",
    "                 torch.mean((D - D_data) ** 2))\n",
    "    \n",
    "    pde_loss = (torch.mean(pde_S ** 2) + \n",
    "                torch.mean(pde_A ** 2) + \n",
    "                torch.mean(pde_I ** 2) + \n",
    "                torch.mean(pde_R ** 2) + \n",
    "                torch.mean(pde_D ** 2))\n",
    "    \n",
    "    return data_loss + pde_loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(time, S_data, A_data, I_data, R_data, D_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Beta: {model.beta.item()}, '\n",
    "              f'Gamma_A: {model.gamma_A.item()}, Gamma_I: {model.gamma_I.item()}, '\n",
    "              f'Delta_A: {model.delta_A.item()}, Delta_I: {model.delta_I.item()}')\n",
    "\n",
    "# Display the estimated parameters\n",
    "print(\"\\nFinal Estimated Parameters:\")\n",
    "print(f\"Estimated Beta: {model.beta.item()}\")\n",
    "print(f\"Estimated Gamma_A: {model.gamma_A.item()}\")\n",
    "print(f\"Estimated Gamma_I: {model.gamma_I.item()}\")\n",
    "print(f\"Estimated Delta_A: {model.delta_A.item()}\")\n",
    "print(f\"Estimated Delta_I: {model.delta_I.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "infections = pd.read_csv('China_infections.csv')\n",
    "infections = infections.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "recoveries = pd.read_csv('China_recoveries.csv')\n",
    "recoveries = recoveries.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "deaths = pd.read_csv('China_deaths.csv')\n",
    "deaths = deaths.drop(columns=['Provincial-level regions', '地级行政区或县级行政区', '省级行政区'])\n",
    "\n",
    "# Assume we are focusing on country-level data\n",
    "# country_infections = infections.iloc[1, 1:].values.astype(float)\n",
    "# country_recoveries = recoveries.iloc[1, 1:].values.astype(float)\n",
    "# country_deaths = deaths.iloc[1, 1:].values.astype(float)\n",
    "dates = infections.columns[1:]\n",
    "\n",
    "country_infections = infections.iloc[1:, 1:].sum(axis=0).values.astype(float)\n",
    "country_recoveries = recoveries.iloc[1:, 1:].sum(axis=0).values.astype(float)\n",
    "country_deaths = deaths.iloc[1:, 1:].sum(axis=0).values.astype(float)\n",
    "\n",
    "# Convert the dates to a numerical scale (days since the first date)\n",
    "time = np.arange(len(dates))\n",
    "\n",
    "# Convert data to tensors\n",
    "time_tensor = torch.tensor(time, dtype=torch.float32).reshape(-1, 1)\n",
    "infections_tensor = torch.tensor(country_infections, dtype=torch.float32).reshape(-1, 1)\n",
    "recoveries_tensor = torch.tensor(country_recoveries, dtype=torch.float32).reshape(-1, 1)\n",
    "deaths_tensor = torch.tensor(country_deaths, dtype=torch.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 5)  # Outputs: S, A, I, R, D\n",
    "        )\n",
    "        \n",
    "        # Separate networks for time-varying parameters\n",
    "        self.beta1 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.beta2 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.gamma1 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.gamma2 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.gamma3 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.kappa1 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.kappa2 = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.mu = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "        self.Lambda = nn.Parameter(torch.tensor(1.0))  # Learnable constant\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # Predict S, A, I, R, D\n",
    "        S, A, I, R, D = self.net(t).T\n",
    "        \n",
    "        # Predict parameters as functions of time\n",
    "        beta1 = self.beta1(t)\n",
    "        beta2 = self.beta2(t)\n",
    "        gamma1 = self.gamma1(t)\n",
    "        gamma2 = self.gamma2(t)\n",
    "        gamma3 = self.gamma3(t)\n",
    "        kappa1 = self.kappa1(t)\n",
    "        kappa2 = self.kappa2(t)\n",
    "        mu = self.mu(t)\n",
    "        \n",
    "        return S, A, I, R, D, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_loss(model, t, infections, recoveries, deaths):\n",
    "    S, A, I, R, D, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu = model(t)\n",
    "    \n",
    "    # Compute derivatives using automatic differentiation\n",
    "    S_t = torch.autograd.grad(S.sum(), t, create_graph=True)[0]\n",
    "    A_t = torch.autograd.grad(A.sum(), t, create_graph=True)[0]\n",
    "    I_t = torch.autograd.grad(I.sum(), t, create_graph=True)[0]\n",
    "    R_t = torch.autograd.grad(R.sum(), t, create_graph=True)[0]\n",
    "    D_t = torch.autograd.grad(D.sum(), t, create_graph=True)[0]\n",
    "\n",
    "    # Define the differential equations based on the SAIRD model\n",
    "    f1 = S_t - (model.Lambda - beta1 * S * A - beta2 * S * I - mu * S)\n",
    "    f2 = A_t - (beta1 * S * A - (gamma1 + gamma2 + gamma3) * A - mu * A)\n",
    "    f3 = I_t - (beta2 * S * I + gamma1 * A - (kappa1 + kappa2) * I - mu * I)\n",
    "    f4 = R_t - (gamma2 * A + kappa1 * I - mu * R)\n",
    "    f5 = D_t - (gamma3 * A + kappa2 * I)\n",
    "    \n",
    "    # Data loss (MSE)\n",
    "    data_loss = (I - infections).pow(2).mean() + (R - recoveries).pow(2).mean() + (D - deaths).pow(2).mean()\n",
    "    \n",
    "    # Physics loss (MSE)\n",
    "    physics_loss = f1.pow(2).mean() + f2.pow(2).mean() + f3.pow(2).mean() + f4.pow(2).mean() + f5.pow(2).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    return data_loss + physics_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10762674.0\n",
      "Epoch 100, Loss: 10714780.0\n",
      "Epoch 200, Loss: 10677808.0\n",
      "Epoch 300, Loss: 10642550.0\n",
      "Epoch 400, Loss: 10608230.0\n",
      "Epoch 500, Loss: 10574541.0\n",
      "Epoch 600, Loss: 10541354.0\n",
      "Epoch 700, Loss: 10508589.0\n",
      "Epoch 800, Loss: 10476198.0\n",
      "Epoch 900, Loss: 10444147.0\n",
      "Epoch 1000, Loss: 10412406.0\n",
      "Epoch 1100, Loss: 10380957.0\n",
      "Epoch 1200, Loss: 10349779.0\n",
      "Epoch 1300, Loss: 10318861.0\n",
      "Epoch 1400, Loss: 10288189.0\n",
      "Epoch 1500, Loss: 10257752.0\n",
      "Epoch 1600, Loss: 10227543.0\n",
      "Epoch 1700, Loss: 10197550.0\n",
      "Epoch 1800, Loss: 10167773.0\n",
      "Epoch 1900, Loss: 10138203.0\n",
      "Epoch 2000, Loss: 10108834.0\n",
      "Epoch 2100, Loss: 10079665.0\n",
      "Epoch 2200, Loss: 10050692.0\n",
      "Epoch 2300, Loss: 10021912.0\n",
      "Epoch 2400, Loss: 9993322.0\n",
      "Epoch 2500, Loss: 9964918.0\n",
      "Epoch 2600, Loss: 9936700.0\n",
      "Epoch 2700, Loss: 9908664.0\n",
      "Epoch 2800, Loss: 9880807.0\n",
      "Epoch 2900, Loss: 9853131.0\n",
      "Epoch 3000, Loss: 9825632.0\n",
      "Epoch 3100, Loss: 9798308.0\n",
      "Epoch 3200, Loss: 9771159.0\n",
      "Epoch 3300, Loss: 9744181.0\n",
      "Epoch 3400, Loss: 9717376.0\n",
      "Epoch 3500, Loss: 9690739.0\n",
      "Epoch 3600, Loss: 9664272.0\n",
      "Epoch 3700, Loss: 9637971.0\n",
      "Epoch 3800, Loss: 9611838.0\n",
      "Epoch 3900, Loss: 9585869.0\n",
      "Epoch 4000, Loss: 9560066.0\n",
      "Epoch 4100, Loss: 9534425.0\n",
      "Epoch 4200, Loss: 9508947.0\n",
      "Epoch 4300, Loss: 9483630.0\n",
      "Epoch 4400, Loss: 9458477.0\n",
      "Epoch 4500, Loss: 9433481.0\n",
      "Epoch 4600, Loss: 9408649.0\n",
      "Epoch 4700, Loss: 9383970.0\n",
      "Epoch 4800, Loss: 9359452.0\n",
      "Epoch 4900, Loss: 9335093.0\n",
      "Epoch 5000, Loss: 9310889.0\n",
      "Epoch 5100, Loss: 9286844.0\n",
      "Epoch 5200, Loss: 9262952.0\n",
      "Epoch 5300, Loss: 9239218.0\n",
      "Epoch 5400, Loss: 9215639.0\n",
      "Epoch 5500, Loss: 9192217.0\n",
      "Epoch 5600, Loss: 9168943.0\n",
      "Epoch 5700, Loss: 9145826.0\n",
      "Epoch 5800, Loss: 9122861.0\n",
      "Epoch 5900, Loss: 9100047.0\n",
      "Epoch 6000, Loss: 9077392.0\n",
      "Epoch 6100, Loss: 9054882.0\n",
      "Epoch 6200, Loss: 9032528.0\n",
      "Epoch 6300, Loss: 9010323.0\n",
      "Epoch 6400, Loss: 8988272.0\n",
      "Epoch 6500, Loss: 8966367.0\n",
      "Epoch 6600, Loss: 8944612.0\n",
      "Epoch 6700, Loss: 8923009.0\n",
      "Epoch 6800, Loss: 8901554.0\n",
      "Epoch 6900, Loss: 8880247.0\n",
      "Epoch 7000, Loss: 8859099.0\n",
      "Epoch 7100, Loss: 8838080.0\n",
      "Epoch 7200, Loss: 8817219.0\n",
      "Epoch 7300, Loss: 8796505.0\n",
      "Epoch 7400, Loss: 8775937.0\n",
      "Epoch 7500, Loss: 8755517.0\n",
      "Epoch 7600, Loss: 8735238.0\n",
      "Epoch 7700, Loss: 8715110.0\n",
      "Epoch 7800, Loss: 8695125.0\n",
      "Epoch 7900, Loss: 8675283.0\n",
      "Epoch 8000, Loss: 8655590.0\n",
      "Epoch 8100, Loss: 8636036.0\n",
      "Epoch 8200, Loss: 8616629.0\n",
      "Epoch 8300, Loss: 8597363.0\n",
      "Epoch 8400, Loss: 8578236.0\n",
      "Epoch 8500, Loss: 8559248.0\n",
      "Epoch 8600, Loss: 8540414.0\n",
      "Epoch 8700, Loss: 8521717.0\n",
      "Epoch 8800, Loss: 8503158.0\n",
      "Epoch 8900, Loss: 8484735.0\n",
      "Epoch 9000, Loss: 8466449.0\n",
      "Epoch 9100, Loss: 8448313.0\n",
      "Epoch 9200, Loss: 8430311.0\n",
      "Epoch 9300, Loss: 8412443.0\n",
      "Epoch 9400, Loss: 8394710.0\n",
      "Epoch 9500, Loss: 8377114.0\n",
      "Epoch 9600, Loss: 8359659.0\n",
      "Epoch 9700, Loss: 8342335.0\n",
      "Epoch 9800, Loss: 8325143.5\n",
      "Epoch 9900, Loss: 8308077.0\n",
      "Epoch 10000, Loss: 8291156.0\n",
      "Epoch 10100, Loss: 8274363.5\n",
      "Epoch 10200, Loss: 8257698.5\n",
      "Epoch 10300, Loss: 8241157.0\n",
      "Epoch 10400, Loss: 8224751.0\n",
      "Epoch 10500, Loss: 8208476.0\n",
      "Epoch 10600, Loss: 8192328.5\n",
      "Epoch 10700, Loss: 8176292.5\n",
      "Epoch 10800, Loss: 8160386.5\n",
      "Epoch 10900, Loss: 8144614.0\n",
      "Epoch 11000, Loss: 8128955.5\n",
      "Epoch 11100, Loss: 8113414.5\n",
      "Epoch 11200, Loss: 8097994.0\n",
      "Epoch 11300, Loss: 8082700.5\n",
      "Epoch 11400, Loss: 8067521.5\n",
      "Epoch 11500, Loss: 8052456.0\n",
      "Epoch 11600, Loss: 8037498.5\n",
      "Epoch 11700, Loss: 8022664.5\n",
      "Epoch 11800, Loss: 8007942.0\n",
      "Epoch 11900, Loss: 7993323.0\n",
      "Epoch 12000, Loss: 7978809.5\n",
      "Epoch 12100, Loss: 7964412.0\n",
      "Epoch 12200, Loss: 7950116.0\n",
      "Epoch 12300, Loss: 7935920.5\n",
      "Epoch 12400, Loss: 7921833.0\n",
      "Epoch 12500, Loss: 7907845.5\n",
      "Epoch 12600, Loss: 7893953.0\n",
      "Epoch 12700, Loss: 7880157.0\n",
      "Epoch 12800, Loss: 7866465.0\n",
      "Epoch 12900, Loss: 7852862.0\n",
      "Epoch 13000, Loss: 7839349.5\n",
      "Epoch 13100, Loss: 7825932.5\n",
      "Epoch 13200, Loss: 7812606.0\n",
      "Epoch 13300, Loss: 7799364.5\n",
      "Epoch 13400, Loss: 7786210.0\n",
      "Epoch 13500, Loss: 7773148.0\n",
      "Epoch 13600, Loss: 7760164.5\n",
      "Epoch 13700, Loss: 7747268.5\n",
      "Epoch 13800, Loss: 7734457.0\n",
      "Epoch 13900, Loss: 7721724.0\n",
      "Epoch 14000, Loss: 7709072.0\n",
      "Epoch 14100, Loss: 7696508.0\n",
      "Epoch 14200, Loss: 7684021.0\n",
      "Epoch 14300, Loss: 7671611.0\n",
      "Epoch 14400, Loss: 7659287.5\n",
      "Epoch 14500, Loss: 7647041.0\n",
      "Epoch 14600, Loss: 7634871.5\n",
      "Epoch 14700, Loss: 7622812.0\n",
      "Epoch 14800, Loss: 7610777.0\n",
      "Epoch 14900, Loss: 7598847.0\n",
      "Epoch 15000, Loss: 7586998.5\n",
      "Epoch 15100, Loss: 7575224.5\n",
      "Epoch 15200, Loss: 7563535.0\n",
      "Epoch 15300, Loss: 7551922.5\n",
      "Epoch 15400, Loss: 7540384.5\n",
      "Epoch 15500, Loss: 7528932.5\n",
      "Epoch 15600, Loss: 7517571.5\n",
      "Epoch 15700, Loss: 7506257.0\n",
      "Epoch 15800, Loss: 7495038.0\n",
      "Epoch 15900, Loss: 7483894.5\n",
      "Epoch 16000, Loss: 7472836.0\n",
      "Epoch 16100, Loss: 7461851.0\n",
      "Epoch 16200, Loss: 7450946.0\n",
      "Epoch 16300, Loss: 7440121.0\n",
      "Epoch 16400, Loss: 7429370.5\n",
      "Epoch 16500, Loss: 7418703.5\n",
      "Epoch 16600, Loss: 7408111.0\n",
      "Epoch 16700, Loss: 7397610.5\n",
      "Epoch 16800, Loss: 7387165.5\n",
      "Epoch 16900, Loss: 7376803.0\n",
      "Epoch 17000, Loss: 7366527.0\n",
      "Epoch 17100, Loss: 7356333.0\n",
      "Epoch 17200, Loss: 7346211.0\n",
      "Epoch 17300, Loss: 7336169.0\n",
      "Epoch 17400, Loss: 7326196.0\n",
      "Epoch 17500, Loss: 7316314.5\n",
      "Epoch 17600, Loss: 7306505.5\n",
      "Epoch 17700, Loss: 7296770.5\n",
      "Epoch 17800, Loss: 7287115.0\n",
      "Epoch 17900, Loss: 7277544.5\n",
      "Epoch 18000, Loss: 7268048.0\n",
      "Epoch 18100, Loss: 7258621.5\n",
      "Epoch 18200, Loss: 7249280.0\n",
      "Epoch 18300, Loss: 7240019.0\n",
      "Epoch 18400, Loss: 7230830.5\n",
      "Epoch 18500, Loss: 7221713.0\n",
      "Epoch 18600, Loss: 7212687.0\n",
      "Epoch 18700, Loss: 7203734.0\n",
      "Epoch 18800, Loss: 7194853.0\n",
      "Epoch 18900, Loss: 7186052.0\n",
      "Epoch 19000, Loss: 7177333.5\n",
      "Epoch 19100, Loss: 7168684.5\n",
      "Epoch 19200, Loss: 7160111.5\n",
      "Epoch 19300, Loss: 7151625.0\n",
      "Epoch 19400, Loss: 7143210.0\n",
      "Epoch 19500, Loss: 7134865.5\n",
      "Epoch 19600, Loss: 7126611.0\n",
      "Epoch 19700, Loss: 7118426.0\n",
      "Epoch 19800, Loss: 7110314.0\n",
      "Epoch 19900, Loss: 7102289.0\n",
      "Epoch 20000, Loss: 7094333.5\n",
      "Epoch 20100, Loss: 7086452.5\n",
      "Epoch 20200, Loss: 7078656.5\n",
      "Epoch 20300, Loss: 7070929.5\n",
      "Epoch 20400, Loss: 7063280.0\n",
      "Epoch 20500, Loss: 7055710.0\n",
      "Epoch 20600, Loss: 7048210.5\n",
      "Epoch 20700, Loss: 7040793.5\n",
      "Epoch 20800, Loss: 7033449.5\n",
      "Epoch 20900, Loss: 7026178.5\n",
      "Epoch 21000, Loss: 7018994.0\n",
      "Epoch 21100, Loss: 7011871.5\n",
      "Epoch 21200, Loss: 7004842.5\n",
      "Epoch 21300, Loss: 6997868.5\n",
      "Epoch 21400, Loss: 6990978.0\n",
      "Epoch 21500, Loss: 6984166.0\n",
      "Epoch 21600, Loss: 6977425.0\n",
      "Epoch 21700, Loss: 6970766.0\n",
      "Epoch 21800, Loss: 6964173.0\n",
      "Epoch 21900, Loss: 6957664.5\n",
      "Epoch 22000, Loss: 6951222.5\n",
      "Epoch 22100, Loss: 6944864.5\n",
      "Epoch 22200, Loss: 6938581.5\n",
      "Epoch 22300, Loss: 6932363.0\n",
      "Epoch 22400, Loss: 6926222.0\n",
      "Epoch 22500, Loss: 6920161.5\n",
      "Epoch 22600, Loss: 6914169.0\n",
      "Epoch 22700, Loss: 6908256.5\n",
      "Epoch 22800, Loss: 6902413.5\n",
      "Epoch 22900, Loss: 6896650.5\n",
      "Epoch 23000, Loss: 6890954.0\n",
      "Epoch 23100, Loss: 6885334.5\n",
      "Epoch 23200, Loss: 6879791.0\n",
      "Epoch 23300, Loss: 6874317.0\n",
      "Epoch 23400, Loss: 6868921.0\n",
      "Epoch 23500, Loss: 6863596.5\n",
      "Epoch 23600, Loss: 6858383.0\n",
      "Epoch 23700, Loss: 6853166.5\n",
      "Epoch 23800, Loss: 6848061.5\n",
      "Epoch 23900, Loss: 6843084.5\n",
      "Epoch 24000, Loss: 6838069.0\n",
      "Epoch 24100, Loss: 6833182.0\n",
      "Epoch 24200, Loss: 6828366.0\n",
      "Epoch 24300, Loss: 6823623.0\n",
      "Epoch 24400, Loss: 6819002.5\n",
      "Epoch 24500, Loss: 6814356.0\n",
      "Epoch 24600, Loss: 6809840.5\n",
      "Epoch 24700, Loss: 6805373.5\n",
      "Epoch 24800, Loss: 6800991.5\n",
      "Epoch 24900, Loss: 6796698.0\n",
      "Epoch 25000, Loss: 6792438.5\n",
      "Epoch 25100, Loss: 6788319.0\n",
      "Epoch 25200, Loss: 6784169.5\n",
      "Epoch 25300, Loss: 6780141.5\n",
      "Epoch 25400, Loss: 6776183.0\n",
      "Epoch 25500, Loss: 6772294.0\n",
      "Epoch 25600, Loss: 6768477.0\n",
      "Epoch 25700, Loss: 6764729.0\n",
      "Epoch 25800, Loss: 6761051.0\n",
      "Epoch 25900, Loss: 6757441.5\n",
      "Epoch 26000, Loss: 6753901.0\n",
      "Epoch 26100, Loss: 6750433.5\n",
      "Epoch 26200, Loss: 6747026.0\n",
      "Epoch 26300, Loss: 6743691.5\n",
      "Epoch 26400, Loss: 6740427.0\n",
      "Epoch 26500, Loss: 6737228.5\n",
      "Epoch 26600, Loss: 6734099.0\n",
      "Epoch 26700, Loss: 6731035.0\n",
      "Epoch 26800, Loss: 6728040.0\n",
      "Epoch 26900, Loss: 6725111.0\n",
      "Epoch 27000, Loss: 6722248.0\n",
      "Epoch 27100, Loss: 6719451.0\n",
      "Epoch 27200, Loss: 6716720.5\n",
      "Epoch 27300, Loss: 6714055.0\n",
      "Epoch 27400, Loss: 6711456.0\n",
      "Epoch 27500, Loss: 6708920.5\n",
      "Epoch 27600, Loss: 6706472.5\n",
      "Epoch 27700, Loss: 6704045.0\n",
      "Epoch 27800, Loss: 6701702.0\n",
      "Epoch 27900, Loss: 6699423.5\n",
      "Epoch 28000, Loss: 6697206.5\n",
      "Epoch 28100, Loss: 6695114.5\n",
      "Epoch 28200, Loss: 6692962.0\n",
      "Epoch 28300, Loss: 6690931.5\n",
      "Epoch 28400, Loss: 6688964.5\n",
      "Epoch 28500, Loss: 6687056.5\n",
      "Epoch 28600, Loss: 6685214.5\n",
      "Epoch 28700, Loss: 6683422.5\n",
      "Epoch 28800, Loss: 6681693.5\n",
      "Epoch 28900, Loss: 6680025.0\n",
      "Epoch 29000, Loss: 6678412.5\n",
      "Epoch 29100, Loss: 6676863.5\n",
      "Epoch 29200, Loss: 6675362.0\n",
      "Epoch 29300, Loss: 6673979.0\n",
      "Epoch 29400, Loss: 6672537.5\n",
      "Epoch 29500, Loss: 6671207.0\n",
      "Epoch 29600, Loss: 6669931.5\n",
      "Epoch 29700, Loss: 6668710.0\n",
      "Epoch 29800, Loss: 6667540.5\n",
      "Epoch 29900, Loss: 6666424.5\n",
      "Epoch 30000, Loss: 6665358.5\n",
      "Epoch 30100, Loss: 6664343.5\n",
      "Epoch 30200, Loss: 6663377.5\n",
      "Epoch 30300, Loss: 6662461.0\n",
      "Epoch 30400, Loss: 6661592.0\n",
      "Epoch 30500, Loss: 6660769.0\n",
      "Epoch 30600, Loss: 6659993.0\n",
      "Epoch 30700, Loss: 6659261.0\n",
      "Epoch 30800, Loss: 6658572.0\n",
      "Epoch 30900, Loss: 6657927.0\n",
      "Epoch 31000, Loss: 6657323.5\n",
      "Epoch 31100, Loss: 6656759.5\n",
      "Epoch 31200, Loss: 6656233.5\n",
      "Epoch 31300, Loss: 6655747.5\n",
      "Epoch 31400, Loss: 6655297.5\n",
      "Epoch 31500, Loss: 6654881.5\n",
      "Epoch 31600, Loss: 6654500.0\n",
      "Epoch 31700, Loss: 6654150.5\n",
      "Epoch 31800, Loss: 6653837.0\n",
      "Epoch 31900, Loss: 6653543.0\n",
      "Epoch 32000, Loss: 6653281.5\n",
      "Epoch 32100, Loss: 6653047.0\n",
      "Epoch 32200, Loss: 6652836.5\n",
      "Epoch 32300, Loss: 6652662.0\n",
      "Epoch 32400, Loss: 6652484.5\n",
      "Epoch 32500, Loss: 6652340.0\n",
      "Epoch 32600, Loss: 6652220.0\n",
      "Epoch 32700, Loss: 6652103.5\n",
      "Epoch 32800, Loss: 6652009.0\n",
      "Epoch 32900, Loss: 6651932.5\n",
      "Epoch 33000, Loss: 6651861.5\n",
      "Epoch 33100, Loss: 6651805.0\n",
      "Epoch 33200, Loss: 6651757.5\n",
      "Epoch 33300, Loss: 6651719.0\n",
      "Epoch 33400, Loss: 6651707.0\n",
      "Epoch 33500, Loss: 6651663.0\n",
      "Epoch 33600, Loss: 6651643.0\n",
      "Epoch 33700, Loss: 6651628.0\n",
      "Epoch 33800, Loss: 6651616.0\n",
      "Epoch 33900, Loss: 6651608.0\n",
      "Epoch 34000, Loss: 6651601.0\n",
      "Epoch 34100, Loss: 6651595.5\n",
      "Epoch 34200, Loss: 6651592.5\n",
      "Epoch 34300, Loss: 6651589.5\n",
      "Epoch 34400, Loss: 6651596.0\n",
      "Epoch 34500, Loss: 6651587.5\n",
      "Epoch 34600, Loss: 6651586.5\n",
      "Epoch 34700, Loss: 6651587.0\n",
      "Epoch 34800, Loss: 6651586.0\n",
      "Epoch 34900, Loss: 6651602.5\n",
      "Epoch 35000, Loss: 6651586.0\n",
      "Epoch 35100, Loss: 6651585.0\n",
      "Epoch 35200, Loss: 6651586.5\n",
      "Epoch 35300, Loss: 6651586.0\n",
      "Epoch 35400, Loss: 6651585.0\n",
      "Epoch 35500, Loss: 6651585.0\n",
      "Epoch 35600, Loss: 6651586.0\n",
      "Epoch 35700, Loss: 6651586.0\n",
      "Epoch 35800, Loss: 6651585.0\n",
      "Epoch 35900, Loss: 6651586.0\n",
      "Epoch 36000, Loss: 6651586.0\n",
      "Epoch 36100, Loss: 6651585.0\n",
      "Epoch 36200, Loss: 6651585.0\n",
      "Epoch 36300, Loss: 6651585.0\n",
      "Epoch 36400, Loss: 6651585.0\n",
      "Epoch 36500, Loss: 6651586.0\n",
      "Epoch 36600, Loss: 6651587.5\n",
      "Epoch 36700, Loss: 6651585.0\n",
      "Epoch 36800, Loss: 6651585.0\n",
      "Epoch 36900, Loss: 6651585.0\n",
      "Epoch 37000, Loss: 6651585.0\n",
      "Epoch 37100, Loss: 6651585.0\n",
      "Epoch 37200, Loss: 6651586.0\n",
      "Epoch 37300, Loss: 6651657.0\n",
      "Epoch 37400, Loss: 6651585.0\n",
      "Epoch 37500, Loss: 6651585.0\n",
      "Epoch 37600, Loss: 6651586.0\n",
      "Epoch 37700, Loss: 6651586.5\n",
      "Epoch 37800, Loss: 6651586.5\n",
      "Epoch 37900, Loss: 6651615.5\n",
      "Epoch 38000, Loss: 6651585.0\n",
      "Epoch 38100, Loss: 6651585.0\n",
      "Epoch 38200, Loss: 6651585.5\n",
      "Epoch 38300, Loss: 6651585.0\n",
      "Epoch 38400, Loss: 6651585.0\n",
      "Epoch 38500, Loss: 6651585.5\n",
      "Epoch 38600, Loss: 6651586.0\n",
      "Epoch 38700, Loss: 6651593.0\n",
      "Epoch 38800, Loss: 6651585.0\n",
      "Epoch 38900, Loss: 6651585.0\n",
      "Epoch 39000, Loss: 6651585.0\n",
      "Epoch 39100, Loss: 6651586.0\n",
      "Epoch 39200, Loss: 6651585.0\n",
      "Epoch 39300, Loss: 6651585.0\n",
      "Epoch 39400, Loss: 6651592.0\n",
      "Epoch 39500, Loss: 6651585.0\n",
      "Epoch 39600, Loss: 6651585.0\n",
      "Epoch 39700, Loss: 6651586.0\n",
      "Epoch 39800, Loss: 6651585.0\n",
      "Epoch 39900, Loss: 6651585.5\n",
      "Epoch 40000, Loss: 6651585.0\n",
      "Epoch 40100, Loss: 6651585.0\n",
      "Epoch 40200, Loss: 6651585.0\n",
      "Epoch 40300, Loss: 6651585.0\n",
      "Epoch 40400, Loss: 6651644.0\n",
      "Epoch 40500, Loss: 6651585.0\n",
      "Epoch 40600, Loss: 6651604.5\n",
      "Epoch 40700, Loss: 6651585.0\n",
      "Epoch 40800, Loss: 6651643.0\n",
      "Epoch 40900, Loss: 6651586.0\n",
      "Epoch 41000, Loss: 6651586.0\n",
      "Epoch 41100, Loss: 6651585.0\n",
      "Epoch 41200, Loss: 6651585.0\n",
      "Epoch 41300, Loss: 6651585.0\n",
      "Epoch 41400, Loss: 6651585.0\n",
      "Epoch 41500, Loss: 6651585.0\n",
      "Epoch 41600, Loss: 6651585.0\n",
      "Epoch 41700, Loss: 6651585.0\n",
      "Epoch 41800, Loss: 6651585.0\n",
      "Epoch 41900, Loss: 6651585.0\n",
      "Epoch 42000, Loss: 6651585.0\n",
      "Epoch 42100, Loss: 6651598.5\n",
      "Epoch 42200, Loss: 6651585.0\n",
      "Epoch 42300, Loss: 6651585.0\n",
      "Epoch 42400, Loss: 6651585.5\n",
      "Epoch 42500, Loss: 6651585.0\n",
      "Epoch 42600, Loss: 6651612.5\n",
      "Epoch 42700, Loss: 6651585.0\n",
      "Epoch 42800, Loss: 6651585.0\n",
      "Epoch 42900, Loss: 6651586.5\n",
      "Epoch 43000, Loss: 6651586.0\n",
      "Epoch 43100, Loss: 6651640.0\n",
      "Epoch 43200, Loss: 6651585.0\n",
      "Epoch 43300, Loss: 6651585.0\n",
      "Epoch 43400, Loss: 6651585.0\n",
      "Epoch 43500, Loss: 6651585.0\n",
      "Epoch 43600, Loss: 6651595.5\n",
      "Epoch 43700, Loss: 6651585.0\n",
      "Epoch 43800, Loss: 6651585.0\n",
      "Epoch 43900, Loss: 6651589.5\n",
      "Epoch 44000, Loss: 6651585.0\n",
      "Epoch 44100, Loss: 6651585.0\n",
      "Epoch 44200, Loss: 6651585.0\n",
      "Epoch 44300, Loss: 6651586.0\n",
      "Epoch 44400, Loss: 6651598.5\n",
      "Epoch 44500, Loss: 6651586.0\n",
      "Epoch 44600, Loss: 6651588.5\n",
      "Epoch 44700, Loss: 6651586.0\n",
      "Epoch 44800, Loss: 6651586.0\n",
      "Epoch 44900, Loss: 6651585.0\n",
      "Epoch 45000, Loss: 6651585.0\n",
      "Epoch 45100, Loss: 6651585.0\n",
      "Epoch 45200, Loss: 6651585.0\n",
      "Epoch 45300, Loss: 6651604.5\n",
      "Epoch 45400, Loss: 6651586.0\n",
      "Epoch 45500, Loss: 6651586.0\n",
      "Epoch 45600, Loss: 6651586.0\n",
      "Epoch 45700, Loss: 6651586.0\n",
      "Epoch 45800, Loss: 6651596.5\n",
      "Epoch 45900, Loss: 6651586.0\n",
      "Epoch 46000, Loss: 6651586.0\n",
      "Epoch 46100, Loss: 6651586.0\n",
      "Epoch 46200, Loss: 6651586.0\n",
      "Epoch 46300, Loss: 6651586.0\n",
      "Epoch 46400, Loss: 6651585.0\n",
      "Epoch 46500, Loss: 6651585.0\n",
      "Epoch 46600, Loss: 6651586.0\n",
      "Epoch 46700, Loss: 6651585.0\n",
      "Epoch 46800, Loss: 6651585.0\n",
      "Epoch 46900, Loss: 6651585.0\n",
      "Epoch 47000, Loss: 6651586.0\n",
      "Epoch 47100, Loss: 6651601.0\n",
      "Epoch 47200, Loss: 6651586.0\n",
      "Epoch 47300, Loss: 6651586.0\n",
      "Epoch 47400, Loss: 6651585.0\n",
      "Epoch 47500, Loss: 6651585.0\n",
      "Epoch 47600, Loss: 6651585.5\n",
      "Epoch 47700, Loss: 6651586.0\n",
      "Epoch 47800, Loss: 6651608.5\n",
      "Epoch 47900, Loss: 6651585.0\n",
      "Epoch 48000, Loss: 6651586.0\n",
      "Epoch 48100, Loss: 6651585.0\n",
      "Epoch 48200, Loss: 6651585.0\n",
      "Epoch 48300, Loss: 6651585.0\n",
      "Epoch 48400, Loss: 6651585.0\n",
      "Epoch 48500, Loss: 6651586.0\n",
      "Epoch 48600, Loss: 6651586.0\n",
      "Epoch 48700, Loss: 6651586.0\n",
      "Epoch 48800, Loss: 6651586.0\n",
      "Epoch 48900, Loss: 6651586.0\n",
      "Epoch 49000, Loss: 6651597.5\n",
      "Epoch 49100, Loss: 6651586.0\n",
      "Epoch 49200, Loss: 6651586.0\n",
      "Epoch 49300, Loss: 6651585.0\n",
      "Epoch 49400, Loss: 6651586.0\n",
      "Epoch 49500, Loss: 6651587.0\n",
      "Epoch 49600, Loss: 6651585.0\n",
      "Epoch 49700, Loss: 6651586.0\n",
      "Epoch 49800, Loss: 6651585.0\n",
      "Epoch 49900, Loss: 6651585.0\n"
     ]
    }
   ],
   "source": [
    "model = PINN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Enable gradient tracking for time\n",
    "time_tensor.requires_grad = True\n",
    "\n",
    "# Training loop\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = pinn_loss(model, time_tensor, infections_tensor, recoveries_tensor, deaths_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m S, A, I, R, D, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu \u001b[38;5;241m=\u001b[39m model(time_tensor)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(time, I, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Infections\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActual Infections\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime (days)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfections\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_base.py:483\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    481\u001b[0m     axes\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mupdate_units(x)\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes\u001b[38;5;241m.\u001b[39myaxis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m     \u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axis.py:1756\u001b[0m, in \u001b[0;36mAxis.update_units\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1754\u001b[0m neednew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m!=\u001b[39m converter\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m converter\n\u001b[1;32m-> 1756\u001b[0m default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_units(default)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\category.py:105\u001b[0m, in \u001b[0;36mStrCategoryConverter.default_units\u001b[1;34m(data, axis)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# the conversion call stack is default_units -> axis_info -> convert\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_units(\u001b[43mUnitData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     axis\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m.\u001b[39mupdate(data)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\category.py:181\u001b[0m, in \u001b[0;36mUnitData.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\category.py:214\u001b[0m, in \u001b[0;36mUnitData.update\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# check if convertible to number:\u001b[39;00m\n\u001b[0;32m    213\u001b[0m convertible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mOrderedDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# OrderedDict just iterates over unique values in data.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     _api\u001b[38;5;241m.\u001b[39mcheck_isinstance((\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m), value\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convertible:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;66;03m# this will only be called so long as convertible is True.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    S, A, I, R, D, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu = model(time_tensor)\n",
    "    plt.plot(time, I, label='Predicted Infections')\n",
    "    plt.plot(time, infections, label='Actual Infections')\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Infections')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
