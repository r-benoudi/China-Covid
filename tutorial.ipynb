{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINN Tutorial --- COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will show the entire process of training a neural network to learn a disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the following system of differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dS/dt = - (alpha / N)  S  I\n",
    "\n",
    "dI/dt = (alpha / N)  S  I - beta  I - gamma  I \n",
    "\n",
    "dD/dt = gamma  I\n",
    "\n",
    "dR/dt = beta  I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Where \n",
    "\n",
    "beta = “effective/apparent” per day recovery rates\n",
    "\n",
    "gamma = “effective/apparent” per day fatality rates\n",
    "\n",
    "alpha = infection rate\n",
    "\n",
    "N = population size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This system represents a COVID model and has 4 compartments: susceptible, infected, dead, and recovered\n",
    "\n",
    "## Since we don't have actual data from the environment to work with, we will generate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries for generating & visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start by setting some information that we got from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A grid of time points (in days)\n",
    "t = np.linspace(0, 500, 100) #from day 0 to day 500, generate 100 points\n",
    "\n",
    "# Initial conditions (example values)\n",
    "N = 1.4e9  # Total population\n",
    "I0 = 1e4  # Initial infected cases\n",
    "A0 = 0.3 * I0  # Initial asymptomatic cases (30% of initial infections)\n",
    "R0 = 0  # Initial recovered cases\n",
    "D0 = 0  # Initial deceased cases\n",
    "# S0 = N - I0 - A0 - R0 - D0  # Initial susceptible population\n",
    "S0 = 1e6\n",
    "\n",
    "# Initial state\n",
    "y0 = [S0, A0, I0, R0, D0]\n",
    "\n",
    "# Parameters (example values, adjust based on estimates)\n",
    "Lambda = 0  # No birth rate considered in this example\n",
    "beta1 = 0.2\n",
    "beta2 = 0.3\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.05\n",
    "gamma3 = 0.01\n",
    "kappa1 = 0.02\n",
    "kappa2 = 0.01\n",
    "mu = 0.0001  # Natural death rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then write out the system of equations in a function that will calculate the value of each compartment at each time step (e.g day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SIR model differential equations.\n",
    "def deriv(y, t, Lambda, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu):\n",
    "    S, A, I, R, D = y\n",
    "    \n",
    "    # Define the system of differential equations\n",
    "    dS_dt = Lambda - beta1 * S * A - beta2 * S * I - mu * S\n",
    "    dA_dt = beta1 * S * A - (gamma1 + gamma2 + gamma3) * A - mu * A\n",
    "    dI_dt = beta2 * S * I + gamma1 * A - (kappa1 + kappa2) * I - mu * I\n",
    "    dR_dt = gamma2 * A + kappa1 * I - mu * R\n",
    "    dD_dt = gamma3 * A + kappa2 * I\n",
    "    \n",
    "    return [dS_dt, dA_dt, dI_dt, dR_dt, dD_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then we pass the initial conditions to the function to get the values of each compartment for the length we chose before (500 days, 100 data points per compartment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions vector\n",
    "# y0 = S0, I0, D0, R0\n",
    "# Integrate the SIR equations over the time grid, t.\n",
    "solution = odeint(deriv, y0, t, args=(Lambda, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu))\n",
    "S, A, I, R, D = solution.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "ax.plot(t, S, 'violet', alpha=0.5, lw=2, label='Susceptible', linestyle='solid')\n",
    "ax.plot(t, I, 'darkgreen', alpha=0.5, lw=2, label='Infected', linestyle='dashed')\n",
    "ax.plot(t, D, 'blue', alpha=0.5, lw=2, label='Dead', linestyle='dashed')\n",
    "ax.plot(t, R, 'red', alpha=0.5, lw=2, label='Recovered', linestyle='dashed')\n",
    "\n",
    "ax.set_xlabel('Time /days')\n",
    "ax.set_ylabel('Number')\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.xaxis.set_tick_params(length=0)\n",
    "ax.grid(visible=True, which='major', c='black', lw=0.2, ls='-')\n",
    "legend = ax.legend()\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "for spine in ('top', 'right', 'bottom', 'left'):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And to save the results as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file\n",
    "COVID_Data = np.asarray([t, S, A, I, D, R]) \n",
    "\n",
    "np.savetxt(\"COVID_Tutorial.csv\", COVID_Data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great! Now we have data to work with. On to training a neural network.\n",
    "I will break down the entire training process into parts. These parts might NOT be runnable on their own (meaning, you can't run the code -- which is why I will comment them out). The point is, I think that it will be easier to understand the process by first understanding what each part is doing, and then seeing how they all work together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a3c696c550>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1234) #set seed (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = genfromtxt('COVID_Tutorial.csv', delimiter=',') #in the form of [t, S, A, I, D, R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): # remember that the data was saved as [t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        # here all the \"loading the data\" and training is happening\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Now we need to define some initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. There are two options for this part. Either you know the values of the parameters (alpha, beta, gamma) or you don't. \n",
    "If you're wondering how can we not know their values if we just generated the data using them, remember that we can also get the data from the environment, and so we might not have the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we don't know their values, we let the neural network learn it\n",
    "Note that the reason for the \"tilda\" part will be clear soon. It's basically to bound the variables around a certain range (imagine telling the neural network to learn alpha, but not from negative infinity to infinity, but from -100 to 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "# self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "# self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we do know their values, we just set it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.alpha_tilda = torch.tensor(0.191)\n",
    "# self.beta_tilda = torch.tensor(0.05)\n",
    "# self.gamma_tilda = torch.tensor(0.0294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's assume we don't know them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "        Lambda, beta1, beta2, gamma1, gamma2, gamma3, kappa1, kappa2, mu\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalize the data\n",
    "Remember that our population size was 59 mil? Remember that there was only 1 infected person and almost 59 mil susceptible people? These drastic variations in values are quite challenging for the network to learn. So we normalize each compartment to be between 0 and 1 for the sake of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find values for normalization\n",
    "\n",
    "# max values\n",
    "# self.S_max = max(self.S)\n",
    "# self.I_max = max(self.I)\n",
    "# self.D_max = max(self.D)\n",
    "# self.R_max = max(self.R)\n",
    "\n",
    "# min values\n",
    "# self.S_min = min(self.S)\n",
    "# self.I_min = min(self.I)\n",
    "# self.D_min = min(self.D)\n",
    "# self.R_min = min(self.R)\n",
    "\n",
    "# create new normalized parameters (which is why the \"hat\" parts)\n",
    "# self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "# self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "# self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "# self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A bit \"hacky\" way to calculate the gradients later\n",
    "It doesn't seem like PyTorch has an easy way to calculate gradients. The issue is that \"grad\" only knows how to propagate gradients from a scalar tensor (which our network's output is not), which is why I had to calculate the Jacobian.\n",
    "So instead of calculating the entire jacobian, I'm using this \"hacky\" way\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrices (x4 for S,I,D,R) for the gradients\n",
    "# What's important here:\n",
    "# We have 4 compartments, hence the value 4 in \"torch.zeros((len(self.t), 4))\". \n",
    "# If we had 20 compartments we would write torch.zeros((len(self.t), 20))\n",
    "# Also, we're setting each specific column in the formed matrices to 1\n",
    "\n",
    "# self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1\n",
    "# self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1\n",
    "# self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1\n",
    "# self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1\n",
    "\n",
    "# See (https://stackoverflow.com/questions/67472361/using-pytorchs-autograd-efficiently-with-tensors-by-calculating-the-jacobian) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Let's initialize the network and learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initializing the neural network\n",
    "# self.net_sidr = self.Net_sidr()\n",
    "\n",
    "# # adding the parameters (alpha, beta, gamma) to the list of learnable parameters (basically, without this part only the neural network's weights will be updated, so we're telling the model to learn alpha, beta, and gamma as well)\n",
    "# self.params = list(self.net_sidr.parameters())\n",
    "# self.params.extend(list([self.alpha_tilda, self.beta_tilda, self.gamma_tilda]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Forcing the parameters to be in a certain range\n",
    "As I mentioned before, we could let the model learn the parameters from negative infinity to infinity, but why should we? These parameters usually have some plausible range. Here we force these ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#force parameters to be in the range of (-1, 1)\n",
    "@property\n",
    "def alpha(self):\n",
    "    return torch.tanh(self.alpha_tilda) \n",
    "\n",
    "@property\n",
    "def beta(self):\n",
    "    return torch.tanh(self.beta_tilda) \n",
    "\n",
    "@property\n",
    "def gamma(self):\n",
    "    return torch.tanh(self.gamma_tilda) \n",
    "\n",
    "\n",
    "#note that you can easily play with that:\n",
    "\n",
    "#force parameters to be in various ranges\n",
    "@property\n",
    "def alpha(self):\n",
    "    return torch.tanh(self.alpha_tilda) * 0.5 # range of (-0.5, 0.5)\n",
    "\n",
    "@property\n",
    "def beta(self):\n",
    "    return torch.tanh(self.beta_tilda) * 0.01 + 1 # range of (-0.99, 1.01)\n",
    "\n",
    "@property\n",
    "def gamma(self):\n",
    "    return torch.tanh(self.gamma_tilda) * 100 # range of (-100, 100)\n",
    "\n",
    "\n",
    "# Also note that we call these alpha, beta, and gamma (in comparison to \"alpha_tilda\", etc. from before)\n",
    "\n",
    "# If you know the values of the parameters you just need to change this to:\n",
    "@property\n",
    "def alpha(self):\n",
    "    return self.alpha_tilda\n",
    "\n",
    "@property\n",
    "def beta(self):\n",
    "    return self.beta_tilda\n",
    "\n",
    "@property\n",
    "def gamma(self):\n",
    "    return self.gamma_tilda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "    def __init__(self):\n",
    "        super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "        self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "        self.fc2=nn.Linear(20, 20)\n",
    "        self.fc3=nn.Linear(20, 20)\n",
    "        self.fc4=nn.Linear(20, 20)\n",
    "        self.fc5=nn.Linear(20, 20)\n",
    "        self.fc6=nn.Linear(20, 20)\n",
    "        self.fc7=nn.Linear(20, 20)\n",
    "        self.fc8=nn.Linear(20, 20)\n",
    "        self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "    def forward(self, t_batch):\n",
    "        sidr=F.relu(self.fc1(t_batch))\n",
    "        sidr=F.relu(self.fc2(sidr))\n",
    "        sidr=F.relu(self.fc3(sidr))\n",
    "        sidr=F.relu(self.fc4(sidr))\n",
    "        sidr=F.relu(self.fc5(sidr))\n",
    "        sidr=F.relu(self.fc6(sidr))\n",
    "        sidr=F.relu(self.fc7(sidr))\n",
    "        sidr=F.relu(self.fc8(sidr))\n",
    "        sidr=self.out(sidr)\n",
    "        return sidr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Now the somewhat complicated part, we create another function that takes the timesteps batch, and pass it to the neural network \n",
    "We basically want to optimize the neural network and also this function that has the system of equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. we now want to get the derivative of each compartment with respect to time (this is why we had to do the \"hacky\" jacobian part)\n",
    "We do this to plug in the systems of equations (you'll see in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #S_t\n",
    "# sidr_hat.backward(self.m1, retain_graph=True)\n",
    "# S_hat_t = self.t.grad.clone()\n",
    "# self.t.grad.zero_()\n",
    "\n",
    "# #I_t\n",
    "# sidr_hat.backward(self.m2, retain_graph=True)\n",
    "# I_hat_t = self.t.grad.clone()\n",
    "# self.t.grad.zero_()\n",
    "\n",
    "# #D_t\n",
    "# sidr_hat.backward(self.m3, retain_graph=True)\n",
    "# D_hat_t = self.t.grad.clone()\n",
    "# self.t.grad.zero_()\n",
    "\n",
    "# #R_t\n",
    "# sidr_hat.backward(self.m4, retain_graph=True)\n",
    "# R_hat_t = self.t.grad.clone()\n",
    "# self.t.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #A_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "        A_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "        I_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "        D_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "        R_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. We now unnormalize the compartments because we don't actually want the equations to change, we just wanted the network to learn quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #unnormalize\n",
    "# S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "# I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "# D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "# R = self.R_min + (self.R_max - self.R_min) * R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #A_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "        A_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "        I_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "        D_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "        R_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #unnormalize\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        A = self.A_min + (self.A_max - self.A_min) * A_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Lastly (almost), we write out the systems of equations we want to learn\n",
    "These are almost the same as the initial system of equations, except that we have another normalization component here (e.g \"/ (self.S_max - self.S_min)\"), and also that we use the partial derivates (e.g \"S\" with respect to time) here. We basically moved the right side of each compartment to the left (hence the negative signs after the derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_hat = S_hat_t - (-(self.alpha / self.N) * S * I)  / (self.S_max - self.S_min)\n",
    "# f2_hat = I_hat_t - ((self.alpha / self.N) * S * I - self.beta * I - self.gamma * I ) / (self.I_max - self.I_min)\n",
    "# f3_hat = D_hat_t - (self.gamma * I) / (self.D_max - self.D_min)\n",
    "# f4_hat = R_hat_t - (self.beta * I ) / (self.R_max - self.R_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #A_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "        A_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "        I_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "        D_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "        R_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #unnormalize\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        A = self.A_min + (self.A_max - self.A_min) * A_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat     \n",
    "\n",
    "        dS_dt = Lambda - beta1 * S * A - beta2 * S * I - mu * S\n",
    "        dA_dt = beta1 * S * A - (gamma1 + gamma2 + gamma3) * A - mu * A\n",
    "        dI_dt = beta2 * S * I + gamma1 * A - (kappa1 + kappa2) * I - mu * I\n",
    "        dR_dt = gamma2 * A + kappa1 * I - mu * R\n",
    "        dD_dt = gamma3 * A + kappa2 * I     \n",
    "\n",
    "        f1_hat = S_hat_t - (self.Lambda - self.beta1 * S * A - self.beta2 * S * I - self.mu * S)  / (self.S_max - self.S_min)\n",
    "        f2_hat = A_hat_t - (self.beta1 * S * A - (self.gamma1 + self.gamma2 + self.gamma3) * A - self.mu * A) / (self.A_max - self.A_min)\n",
    "        f3_hat = I_hat_t - (self.beta2 * S * I + self.gamma1 * A - (self.kappa1 + self.kappa2) * I - self.mu * I) / (self.I_max - self.I_min)\n",
    "        f4_hat = D_hat_t - (self.gamma3 * A + self.kappa2 * I) / (self.D_max - self.D_min)\n",
    "        f5_hat = R_hat_t - (self.gamma2 * A + self.kappa1 * I - self.mu * R) / (self.R_max - self.R_min)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Lastly, we return the values we learned and want to optimize --- S, I, D, R, and each system's compartment (e.g f1_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_saidr = self.Net_saidr()\n",
    "        self.params = list(self.net_saidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #A_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "        A_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "        I_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "        D_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "        R_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #unnormalize\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        A = self.A_min + (self.A_max - self.A_min) * A_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat     \n",
    "\n",
    "        dS_dt = Lambda - beta1 * S * A - beta2 * S * I - mu * S\n",
    "        dA_dt = beta1 * S * A - (gamma1 + gamma2 + gamma3) * A - mu * A\n",
    "        dI_dt = beta2 * S * I + gamma1 * A - (kappa1 + kappa2) * I - mu * I\n",
    "        dR_dt = gamma2 * A + kappa1 * I - mu * R\n",
    "        dD_dt = gamma3 * A + kappa2 * I     \n",
    "\n",
    "        f1_hat = S_hat_t - (self.Lambda - self.beta1 * S * A - self.beta2 * S * I - self.mu * S)  / (self.S_max - self.S_min)\n",
    "        f2_hat = A_hat_t - (self.beta1 * S * A - (self.gamma1 + self.gamma2 + self.gamma3) * A - self.mu * A) / (self.A_max - self.A_min)\n",
    "        f3_hat = I_hat_t - (self.beta2 * S * I + self.gamma1 * A - (self.kappa1 + self.kappa2) * I - self.mu * I) / (self.I_max - self.I_min)\n",
    "        f4_hat = D_hat_t - (self.gamma3 * A + self.kappa2 * I) / (self.D_max - self.D_min)\n",
    "        f5_hat = R_hat_t - (self.gamma2 * A + self.kappa1 * I - self.mu * R) / (self.R_max - self.R_min)         \n",
    "\n",
    "        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, S_hat, A_hat, I_hat, D_hat, R_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. The training process:\n",
    "Here we just create a function called \"train\", that will take a number of epochs to train for, and will train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, n_epochs):\n",
    "      # train\n",
    "      print('\\nstarting training...\\n')\n",
    "      \n",
    "      for epoch in range(n_epochs):\n",
    "        # lists to hold the output (maintain only the final epoch)\n",
    "        S_pred_list = []\n",
    "        I_pred_list = []\n",
    "        D_pred_list = []\n",
    "        R_pred_list = []\n",
    "\n",
    "        # we pass the timesteps batch into net_f\n",
    "        f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch) # net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n",
    "        \n",
    "        self.optimizer.zero_grad() #zero grad\n",
    "        \n",
    "        #append the values to plot later (note that we unnormalize them here for plotting)\n",
    "        S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n",
    "        I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n",
    "        D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)\n",
    "        R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n",
    "\n",
    "        #calculate the loss --- MSE of the neural networks output and each compartment\n",
    "        loss = (torch.mean(torch.square(self.S_hat - S_pred))+ \n",
    "                torch.mean(torch.square(self.I_hat - I_pred))+\n",
    "                torch.mean(torch.square(self.D_hat - D_pred))+\n",
    "                torch.mean(torch.square(self.R_hat - R_pred))+\n",
    "                torch.mean(torch.square(f1))+\n",
    "                torch.mean(torch.square(f2))+\n",
    "                torch.mean(torch.square(f3))+\n",
    "                torch.mean(torch.square(f4))\n",
    "                ) \n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step() \n",
    "\n",
    "        # append the loss value (we call \"loss.item()\" because we just want the value of the loss and not the entire computational graph)\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        if epoch % 1000 == 0:          \n",
    "          print('\\nEpoch ', epoch)\n",
    "\n",
    "          print('alpha: (goal 0.191 ', self.alpha)\n",
    "          print('beta: (goal 0.05 ', self.beta)\n",
    "          print('gamma: (goal 0.0294 ', self.gamma)\n",
    "\n",
    "          print('#################################')                \n",
    "\n",
    "      return S_pred_list, I_pred_list, D_pred_list, R_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, A_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = 1.4e9 #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.A = torch.tensor(A_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.Lambda_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma3_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.A_max = max(self.A)\n",
    "        self.I_max = max(self.I)\n",
    "        self.D_max = max(self.D)\n",
    "        self.R_max = max(self.R)\n",
    "        self.S_min = min(self.S)\n",
    "        self.A_min = min(self.A)\n",
    "        self.I_min = min(self.I)\n",
    "        self.D_min = min(self.D)\n",
    "        self.R_min = min(self.R)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.A_hat = (self.A - self.A_min) / (self.A_max - self.A_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "\n",
    "        #matrices (x4 for S,A,I,D,R) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 5)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 5)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 5)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 5)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 5)); self.m5[:, 4] = 1\n",
    "\n",
    "        #NN\n",
    "        self.net_sidr = self.Net_sidr()\n",
    "        self.params = list(self.net_sidr.parameters())\n",
    "        self.params.extend(list([self.Lambda_tilda,self.beta1_tilda,self.beta2_tilda,self.gamma1_tilda,self.gamma2_tilda,self.gamma3_tilda,self.kappa1_tilda,self.kappa2_tilda,self.mu_tilda]))\n",
    "\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        return torch.tanh(self.Lambda_tilda) \n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return torch.tanh(self.beta1_tilda) \n",
    "    \n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return torch.tanh(self.gamma1_tilda) \n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def gamma3(self):\n",
    "        return torch.tanh(self.gamma3_tilda)\n",
    "    @property\n",
    "    def kappa1(self):\n",
    "        return torch.tanh(self.kappa1_tilda) \n",
    "\n",
    "    @property\n",
    "    def kappa2(self):\n",
    "        return torch.tanh(self.kappa2_tilda) \n",
    "    \n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(self.mu_tilda) \n",
    "\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 5) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        \n",
    "        #pass the timesteps batch to the neural network\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "        \n",
    "        #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "        S_hat, A_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #A_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "        A_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "        I_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "        D_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "        R_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #unnormalize\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        A = self.A_min + (self.A_max - self.A_min) * A_hat\n",
    "        I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "        D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "        R = self.R_min + (self.R_max - self.R_min) * R_hat     \n",
    "\n",
    "        dS_dt = Lambda - beta1 * S * A - beta2 * S * I - mu * S\n",
    "        dA_dt = beta1 * S * A - (gamma1 + gamma2 + gamma3) * A - mu * A\n",
    "        dI_dt = beta2 * S * I + gamma1 * A - (kappa1 + kappa2) * I - mu * I\n",
    "        dR_dt = gamma2 * A + kappa1 * I - mu * R\n",
    "        dD_dt = gamma3 * A + kappa2 * I     \n",
    "\n",
    "        f1_hat = S_hat_t - (self.Lambda - self.beta1 * S * A - self.beta2 * S * I - self.mu * S)  / (self.S_max - self.S_min)\n",
    "        f2_hat = A_hat_t - (self.beta1 * S * A - (self.gamma1 + self.gamma2 + self.gamma3) * A - self.mu * A) / (self.A_max - self.A_min)\n",
    "        f3_hat = I_hat_t - (self.beta2 * S * I + self.gamma1 * A - (self.kappa1 + self.kappa2) * I - self.mu * I) / (self.I_max - self.I_min)\n",
    "        f4_hat = D_hat_t - (self.gamma3 * A + self.kappa2 * I) / (self.D_max - self.D_min)\n",
    "        f5_hat = R_hat_t - (self.gamma2 * A + self.kappa1 * I - self.mu * R) / (self.R_max - self.R_min)         \n",
    "\n",
    "        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, S_hat, A_hat, I_hat, D_hat, R_hat\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        # train\n",
    "        print('\\nstarting training...\\n')\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # lists to hold the output (maintain only the final epoch)\n",
    "            S_pred_list = []\n",
    "            A_pred_list = []\n",
    "            I_pred_list = []\n",
    "            D_pred_list = []\n",
    "            R_pred_list = []\n",
    "\n",
    "            # we pass the timesteps batch into net_f\n",
    "            f1, f2, f3, f4, f5, S_pred, A_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch) # net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n",
    "            \n",
    "            self.optimizer.zero_grad() #zero grad\n",
    "            \n",
    "            #append the values to plot later (note that we unnormalize them here for plotting)\n",
    "            S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n",
    "            A_pred_list.append(self.A_min + (self.A_max - self.A_min) * A_pred)\n",
    "            I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n",
    "            D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)\n",
    "            R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n",
    "\n",
    "            #calculate the loss --- MSE of the neural networks output and each compartment\n",
    "            loss = (torch.mean(torch.square(self.S_hat - S_pred))+ \n",
    "                    torch.mean(torch.square(self.A_hat - A_pred))+\n",
    "                    torch.mean(torch.square(self.I_hat - I_pred))+\n",
    "                    torch.mean(torch.square(self.D_hat - D_pred))+\n",
    "                    torch.mean(torch.square(self.R_hat - R_pred))+\n",
    "                    torch.mean(torch.square(f1))+\n",
    "                    torch.mean(torch.square(f2))+\n",
    "                    torch.mean(torch.square(f3))+\n",
    "                    torch.mean(torch.square(f4))+\n",
    "                    torch.mean(torch.square(f5))\n",
    "                    ) \n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step() \n",
    "\n",
    "            # append the loss value (we call \"loss.item()\" because we just want the value of the loss and not the entire computational graph)\n",
    "            self.losses.append(loss.item())\n",
    "\n",
    "            if epoch % 1000 == 0:          \n",
    "                print('\\nEpoch ', epoch)\n",
    "\n",
    "                print('Lambda: (goal 0.191 ', self.Lambda)\n",
    "                print('beta1: (goal 0.05 ', self.beta1)\n",
    "                print('beta2: (goal 0.05 ', self.beta2)\n",
    "                print('gamma1: (goal 0.0294 ', self.gamma1)\n",
    "                print('gamma2: (goal 0.0294 ', self.gamma2)\n",
    "                print('gamma3: (goal 0.0294 ', self.gamma3)\n",
    "                print('kappa1: (goal 0.0294 ', self.kappa1)\n",
    "                print('kappa2: (goal 0.0294 ', self.kappa2)\n",
    "                print('mu: (goal 0.0294 ', self.mu)\n",
    "                print('#################################')                \n",
    "\n",
    "        return S_pred_list, A_pred_list, I_pred_list, D_pred_list, R_pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting training...\n",
      "\n",
      "\n",
      "Epoch  0\n",
      "Lambda: (goal 0.191  tensor([0.3198], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([0.6548], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([0.5102], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([0.6497], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([0.6755], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([0.2722], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([0.5926], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([0.2763], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([0.5762], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  1000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  2000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  3000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  4000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  5000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  6000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  7000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  8000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  9000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  10000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  11000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  12000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  13000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  14000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  15000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  16000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  17000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  18000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  19000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  20000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  21000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  22000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  23000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  24000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  25000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  26000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  27000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  28000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  29000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  30000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  31000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  32000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  33000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  34000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  35000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  36000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  37000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  38000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  39000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  40000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  41000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  42000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  43000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  44000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  45000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  46000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  47000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  48000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n",
      "\n",
      "Epoch  49000\n",
      "Lambda: (goal 0.191  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta1: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "beta2: (goal 0.05  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "gamma3: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa1: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "kappa2: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "mu: (goal 0.0294  tensor([nan], grad_fn=<TanhBackward0>)\n",
      "#################################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:12\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dinn = DINN(covid_data[0], covid_data[1], covid_data[2], covid_data[3], \n",
    "            covid_data[4], covid_data[5]) #in the form of [t,S,I,D,R]\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = optim.Adam(dinn.params, lr = learning_rate)\n",
    "dinn.optimizer = optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "\n",
    "dinn.scheduler = scheduler\n",
    "\n",
    "S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(50000) #train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Plotting the losses and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0, 0.5, 'Loss'),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dinn.losses[0:], color = 'teal')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S_pred_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxkcd:white\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(covid_data[\u001b[38;5;241m0\u001b[39m], covid_data[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpink\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSusceptible\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(covid_data[\u001b[38;5;241m0\u001b[39m], \u001b[43mS_pred_list\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSusceptible Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(covid_data[\u001b[38;5;241m0\u001b[39m], covid_data[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviolet\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfected\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(covid_data[\u001b[38;5;241m0\u001b[39m], I_pred_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdodgerblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfected Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'S_pred_list' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "ax.plot(covid_data[0], covid_data[1], 'pink', alpha=0.5, lw=2, label='Susceptible')\n",
    "ax.plot(covid_data[0], S_pred_list[0].detach().numpy(), 'red', alpha=0.9, lw=2, label='Susceptible Prediction', linestyle='dashed')\n",
    "\n",
    "ax.plot(covid_data[0], covid_data[2], 'violet', alpha=0.5, lw=2, label='Infected')\n",
    "ax.plot(covid_data[0], I_pred_list[0].detach().numpy(), 'dodgerblue', alpha=0.9, lw=2, label='Infected Prediction', linestyle='dashed')\n",
    "\n",
    "ax.plot(covid_data[0], covid_data[3], 'darkgreen', alpha=0.5, lw=2, label='Dead')\n",
    "ax.plot(covid_data[0], D_pred_list[0].detach().numpy(), 'green', alpha=0.9, lw=2, label='Dead Prediction', linestyle='dashed')\n",
    "\n",
    "ax.plot(covid_data[0], covid_data[4], 'blue', alpha=0.5, lw=2, label='Recovered')\n",
    "ax.plot(covid_data[0], R_pred_list[0].detach().numpy(), 'teal', alpha=0.9, lw=2, label='Recovered Prediction', linestyle='dashed')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Time /days')\n",
    "ax.set_ylabel('Number')\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.xaxis.set_tick_params(length=0)\n",
    "ax.grid(b=True, which='major', c='black', lw=0.2, ls='-')\n",
    "legend = ax.legend()\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "for spine in ('top', 'right', 'bottom', 'left'):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
